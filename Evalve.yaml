name: VAE Evaluate Model
description: Evaluates the trained VAE model and generates performance metrics.
inputs:
  - {name: trained_model, type: Model}     # Trained VAE model from train brick
  - {name: data_path, type: Dataset}
  - {name: config, type: String}           # VAE model configuration 
outputs:
  - {name: metrics, type: Metrics}         # Evaluation metrics and reports
  - {name: metrics_json, type: String}     # Evaluation metrics as a JSON string
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse
        import pickle
        import json
        import os
        import numpy as np
        from nesy_factory.VAE import create_vae_model
        from nesy_factory.utils import set_random_seed
        import torch.nn as nn
        import torch.nn.functional as F
        
        class DataWrapper:
          def __init__(self, data_dict):
              self.__dict__.update(data_dict)

        class ProcessedVAEDataset:
            def __init__(self, data, labels, vae_type):
                self.data = data
                self.labels = labels
                self.vae_type = vae_type
                self.class_mapping = {}

            def __len__(self):
                return len(self.data)

            def __getitem__(self, idx):
                return self.data[idx], self.labels[idx]

        def evaluate_vae_model(model, data_loader, device, vae_type='standard_vae', beta=1.0, config=None):
            # Evaluate VAE model and return metrics
            model.eval()
            total_loss = 0
            total_recon_loss = 0
            total_kl_loss = 0
            num_samples = 0
            
            print("Starting VAE model evaluation...")
            with torch.no_grad():
                for batch_idx, (data_batch, labels) in enumerate(data_loader):
                    data_batch = data_batch.to(device)
                    batch_size = data_batch.size(0)
                    
                    try:
                        if vae_type == 'conditional_vae':
                            labels = labels.to(device)
                            outputs = model(data_batch, labels)
                        elif vae_type == 'vq_vae':
                            outputs = model(data_batch)
                        else:
                            outputs = model(data_batch)
                        
                        # Handle different output formats
                        if isinstance(outputs, dict):
                            # Handle dictionary output format
                            if batch_idx == 0:  # Only print once
                                print(f"Dict output keys: {list(outputs.keys())}")
                            recon_batch = outputs.get('reconstruction', outputs.get('recon', outputs.get('x_recon', None)))
                            mu = outputs.get('mu', outputs.get('z_mean', outputs.get('mean', None)))
                            logvar = outputs.get('logvar', outputs.get('z_logvar', outputs.get('log_var', None)))
                            
                            if recon_batch is None:
                                print("Could not find reconstruction in dict output, skipping batch")
                                continue
                            if mu is None:
                                latent_dim = config.get('latent_dim', 128) if config else 128
                                mu = torch.zeros(batch_size, latent_dim).to(device)
                            if logvar is None:
                                logvar = torch.zeros_like(mu)
                                
                        elif isinstance(outputs, (tuple, list)):
                            # Handle tuple/list output format
                            if vae_type == 'vq_vae':
                                if len(outputs) >= 2:
                                    recon_batch, vq_loss = outputs[0], outputs[1]
                                    recon_loss = F.mse_loss(recon_batch, data_batch, reduction='sum')
                                    total_loss += (recon_loss + vq_loss).item()
                                    total_recon_loss += recon_loss.item()
                                    total_kl_loss += vq_loss.item()
                                    num_samples += batch_size
                                    continue
                                else:
                                    print(f"Unexpected VQ-VAE output format: {len(outputs)} outputs")
                                    continue
                            else:
                                if len(outputs) == 3:
                                    recon_batch, mu, logvar = outputs
                                elif len(outputs) == 2:
                                    recon_batch, mu = outputs
                                    logvar = torch.zeros_like(mu)
                                else:
                                    print(f"Unexpected tuple output format: {len(outputs)} outputs")
                                    recon_batch = outputs[0]
                                    latent_dim = config.get('latent_dim', 128) if config else 128
                                    mu = torch.zeros(batch_size, latent_dim).to(device)
                                    logvar = torch.zeros_like(mu)
                        else:
                            print(f"Unexpected output type: {type(outputs)}")
                            continue
                        
                        # Compute VAE losses
                        if vae_type != 'vq_vae':
                            recon_loss = F.mse_loss(recon_batch, data_batch, reduction='sum')
                            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                            
                            if vae_type == 'beta_vae':
                                loss = recon_loss + beta * kl_loss
                            else:
                                loss = recon_loss + kl_loss
                            
                            total_loss += loss.item()
                            total_recon_loss += recon_loss.item()
                            total_kl_loss += kl_loss.item()
                            num_samples += batch_size
                            
                    except Exception as e:
                        print(f"Error in batch {batch_idx}: {e}")
                        if 'outputs' in locals():
                            print(f"Model output type: {type(outputs)}")
                            if isinstance(outputs, dict):
                                print(f"Dict keys: {list(outputs.keys())}")
                        continue
            
            # Calculate average metrics
            avg_loss = total_loss / num_samples if num_samples > 0 else 0
            avg_recon_loss = total_recon_loss / num_samples if num_samples > 0 else 0
            avg_kl_loss = total_kl_loss / num_samples if num_samples > 0 else 0
            
            # Check error threshold if specified
            threshold_exceeded = False
            error_absolute_threshold = config.get('error_absolute_threshold') if config else None
            if error_absolute_threshold is not None:
                threshold_exceeded = avg_loss > error_absolute_threshold
                print(f"Error threshold check: {avg_loss:.4f} > {error_absolute_threshold} = {threshold_exceeded}")
            
            eval_metrics = {
                'vae_loss': avg_loss,
                'reconstruction_loss': avg_recon_loss,
                'kl_loss': avg_kl_loss,
                'num_samples': num_samples,
                'model_type': vae_type,
                'threshold_exceeded': threshold_exceeded
            }
            
            return eval_metrics

        # Parse arguments
        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--data_path', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--metrics', type=str, required=True)
        parser.add_argument('--metrics_json', type=str, required=True)
        args = parser.parse_args()

        print(f"Trained model path: {args.trained_model}")
        print(f"Data path: {args.data_path}")
        print(f"Output path: {args.metrics}")
        
        print("Loading config...")
        print(f"config file is {args.config}")
        try: 
            config = json.loads(args.config)
        except : 
            with open(args.config) as f:
                config = json.load(f)
        print(f"the configs are : {config}")

        # Load trained VAE model
        print("Loading trained VAE model...")
        vae_type = config.get('vae_type', 'standard_vae')
        
        # First load the state dict to infer the actual dimensions
        print(f"Loading state dict from {args.trained_model}")
        state_dict = torch.load(args.trained_model, map_location=torch.device('cpu'))
        
        # Infer input_dim from the saved model weights
        if 'encoder.0.weight' in state_dict:
            actual_input_dim = state_dict['encoder.0.weight'].shape[1]
            print(f"Inferred input_dim from saved weights: {actual_input_dim}")
            # Update config with actual input_dim
            config['input_dim'] = actual_input_dim
        else:
            actual_input_dim = config.get('input_dim', 784)
            print(f"Could not infer input_dim, using config value: {actual_input_dim}")
        
        # Try to create model using the same factory function as training
        try:
            model = create_vae_model(vae_type, config)
            print(f"Created {vae_type} model using factory function")
        except Exception as e:
            print(f"Factory function failed: {e}")
            print("Creating fallback StandardVAE model...")
            
            # Fallback: Create a StandardVAE that matches the saved weights
            class StandardVAE(nn.Module):
                def __init__(self, input_dim, hidden_dim, latent_dim):
                    super(StandardVAE, self).__init__()
                    self.input_dim = input_dim
                    self.hidden_dim = hidden_dim
                    self.latent_dim = latent_dim
                    
                    # Encoder (matching saved weights structure)
                    self.encoder = nn.Sequential(
                        nn.Linear(input_dim, hidden_dim),
                        nn.ReLU(),
                        nn.Linear(hidden_dim, hidden_dim // 2),
                        nn.ReLU()
                    )
                    # Use the naming from saved weights
                    self.mu_layer = nn.Linear(hidden_dim // 2, latent_dim)
                    self.logvar_layer = nn.Linear(hidden_dim // 2, latent_dim)
                    
                    # Decoder (matching saved weights structure)
                    self.decoder = nn.Sequential(
                        nn.Linear(latent_dim, hidden_dim // 2),
                        nn.ReLU(),
                        nn.Linear(hidden_dim // 2, hidden_dim),
                        nn.ReLU(),
                        nn.Linear(hidden_dim, input_dim),
                        nn.Sigmoid()
                    )
                
                def encode(self, x):
                    h = self.encoder(x)
                    mu = self.mu_layer(h)
                    logvar = self.logvar_layer(h)
                    return mu, logvar
                
                def reparameterize(self, mu, logvar):
                    std = torch.exp(0.5 * logvar)
                    eps = torch.randn_like(std)
                    return mu + eps * std
                
                def decode(self, z):
                    return self.decoder(z)
                
                def forward(self, x):
                    mu, logvar = self.encode(x)
                    z = self.reparameterize(mu, logvar)
                    recon = self.decode(z)
                    return {
                        'reconstruction': recon,
                        'mu': mu,
                        'logvar': logvar,
                        'z': z
                    }
            
            model = StandardVAE(
                actual_input_dim,
                config.get('hidden_dim', 512),
                config.get('latent_dim', 128)
            )
            print(f"Created fallback StandardVAE model with input_dim={actual_input_dim}")
        
        # Load the trained weights
        try:
            model.load_state_dict(state_dict, strict=True)
            print(f"Successfully loaded {vae_type} model weights with strict=True")
        except Exception as e:
            print(f"Error loading model weights with strict=True: {e}")
            print("Trying to load with strict=False...")
            try:
                model.load_state_dict(state_dict, strict=False)
                print("Model loaded with strict=False")
            except Exception as e2:
                print(f"Failed to load model even with strict=False: {e2}")
                exit(1)
        
        # Load data
        try:
            with open(args.data_path, "rb") as f:
                data_loader = pickle.load(f)
            print(f"Successfully loaded data. Type: {type(data_loader)}")
        except Exception as e:
            print(f"Error loading data: {e}")
            exit(1)
        
        # Set device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        print(f"Using device: {device}")
        
        # VAE Model Evaluation 
        print("Starting VAE Model Evaluation")
        beta = config.get('beta', 1.0)
        eval_metrics = evaluate_vae_model(model, data_loader, device, vae_type, beta, config)
        print(f"VAE Evaluation Results: {eval_metrics}")
        
        # Ensure output directory exists
        os.makedirs(os.path.dirname(args.metrics), exist_ok=True)
        os.makedirs(os.path.dirname(args.metrics_json), exist_ok=True)

        # Save metrics
        print("Saving evaluation metrics...")
        try:
            with open(args.metrics, 'w') as f:
                json.dump(eval_metrics, f, indent=2)
            print(f"Saved metrics to {args.metrics}")

            with open(args.metrics_json, 'w') as f:
                json.dump(eval_metrics, f, indent=2)
            print(f"Saved metrics JSON to {args.metrics_json}")

        except Exception as e:
            print(f"Error saving metrics: {e}")
            exit(1)

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
