name: preprocess_worldbank_tensor 4
description: Preprocesses World Bank parquet data - handles Inf values, normalizes, builds sequences, outputs torch tensor (3D for Sequential VAE)
inputs:
  - {name: parquet_path, type: String, description: "Path to tensor_values.parquet"}
  - {name: num_features, type: Integer, description: "Number of features (default 16197)", default: "16197"}
  - {name: seq_length, type: Integer, description: "Sequence length (default 25 timesteps)", default: "25"}
outputs:
  - {name: preprocessed_path, type: String, description: "Path to saved preprocessed tensor (.pt file)"}
  - {name: metadata_path, type: String, description: "Path to metadata JSON (shape, features, etc)"}

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet pyarrow pandas numpy torch || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet pyarrow pandas numpy torch --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pyarrow.parquet as pq
        import pandas as pd
        import numpy as np
        import torch

        parser = argparse.ArgumentParser()
        parser.add_argument('--parquet_path', type=str, required=True)
        parser.add_argument('--num_features', type=int, default=16197)
        parser.add_argument('--seq_length', type=int, default=25)
        parser.add_argument('--preprocessed_path', type=str, required=True)
        parser.add_argument('--metadata_path', type=str, required=True)
        args = parser.parse_args()

        print("=" * 70)
        print("PREPROCESSING WORLD BANK TENSOR FOR SEQUENTIAL VAE")
        print("=" * 70)

        try:
            # Step 1: Load parquet
            print("\\n[1/4] Loading parquet file...")

            # Handle Kubeflow artifact directory structure
            parquet_path = args.parquet_path
            if os.path.isdir(parquet_path):
                # Find .parquet file in directory
                files = [f for f in os.listdir(parquet_path) if f.endswith('.parquet')]
                if files:
                    parquet_path = os.path.join(parquet_path, files[0])
                    print(f"   Found parquet file: {parquet_path}")
                else:
                    raise FileNotFoundError(f"No .parquet files found in {parquet_path}")

            print(f"   Reading from: {parquet_path}")
            pf = pq.ParquetFile(parquet_path)
            df = pf.read().to_pandas()
            print(f"   Loaded: {df.shape[0]} rows x {df.shape[1]} columns")

            # Get feature columns
            feature_cols = [c for c in df.columns if c.startswith('I')][:args.num_features]
            print(f"   Using {len(feature_cols)} features")

            # Step 2: Handle Inf values
            print("\\n[2/4] Handling Inf/NaN values...")
            data = df[feature_cols].values.astype(np.float32)
            inf_count = (~np.isfinite(data)).sum()
            data[~np.isfinite(data)] = 0.0
            print(f"   Replaced {inf_count} Inf/NaN values with 0")

            # Step 3: Normalize (z-score)
            print("\\n[3/4] Normalizing features (z-score)...")
            mean_vals = np.mean(data, axis=0, keepdims=True)
            std_vals = np.std(data, axis=0, keepdims=True)
            std_vals[std_vals == 0] = 1.0  # Avoid division by zero
            data_normalized = (data - mean_vals) / std_vals
            print(f"   Mean: [{mean_vals.min():.4f}, {mean_vals.max():.4f}]")
            print(f"   Std:  [{std_vals.min():.4f}, {std_vals.max():.4f}]")

            # Step 4: Build sequences (create temporal groups)
            print("\\n[4/4] Building sequences for temporal modeling...")
            total_rows = len(data_normalized)
            seq_length = args.seq_length

            # Calculate number of sequences
            num_sequences = total_rows // seq_length
            usable_rows = num_sequences * seq_length

            print(f"   Total rows: {total_rows}")
            print(f"   Sequence length: {seq_length}")
            print(f"   Number of sequences: {num_sequences}")
            print(f"   Using: {usable_rows} rows ({usable_rows/total_rows*100:.1f}%)")

            # Reshape into sequences
            sequences = data_normalized[:usable_rows].reshape(num_sequences, seq_length, len(feature_cols))
            print(f"   Reshaped to: {sequences.shape} (samples, timesteps, features)")

            # ------------------------------------------------------------------
            # SAVE OUTPUTS
            # Kubeflow passes a *file path* for each outputPath argument.
            # We must write a plain file to that exact path â€” never turn the
            # path itself into a directory (that causes the "is a directory"
            # copy_file_range error in Argo).
            # ------------------------------------------------------------------
            print("\\n[SAVING] Saving preprocessed tensor...")

            tensor = torch.FloatTensor(sequences)

            # Ensure the parent directory of the output file exists
            preprocessed_parent = os.path.dirname(args.preprocessed_path)
            if preprocessed_parent:
                os.makedirs(preprocessed_parent, exist_ok=True)

            # Write tensor directly to the path Kubeflow provided
            torch.save(tensor, args.preprocessed_path)
            print(f"   Saved Kubeflow artifact to: {args.preprocessed_path}")
            
            # === MINIMAL CHANGE: Also save direct .pt file for easy S3 access ===
            pt_file = args.preprocessed_path + '.pt'
            torch.save(tensor, pt_file)
            print(f"   Direct .pt file saved at: {pt_file}")
            
            print(f"   Shape: {tensor.shape}")
            print(f"   File size (Kubeflow): {os.path.getsize(args.preprocessed_path) / 1e9:.2f} GB")
            print(f"   File size (Direct .pt): {os.path.getsize(pt_file) / 1e9:.2f} GB")

            # Save metadata JSON directly to the path Kubeflow provided
            metadata = {
                "shape": list(tensor.shape),
                "num_samples": tensor.shape[0],
                "seq_length": tensor.shape[1],
                "num_features": tensor.shape[2],
                "features_used": len(feature_cols),
                "inf_values_replaced": int(inf_count),
                "normalization": "z-score (mean=0, std=1)",
                "output_file": args.preprocessed_path,
                "direct_pt_file": pt_file  # === MINIMAL CHANGE: Add direct file to metadata ===
            }

            metadata_parent = os.path.dirname(args.metadata_path)
            if metadata_parent:
                os.makedirs(metadata_parent, exist_ok=True)

            with open(args.metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"   Metadata saved: {args.metadata_path}")

            print("\\n" + "=" * 70)
            print("PREPROCESSING COMPLETE!")
            print("=" * 70)
            print(f"\\nOutput Summary:")
            print(f"  Tensor shape: {tuple(tensor.shape)}")
            print(f"  Samples: {tensor.shape[0]}")
            print(f"  Timesteps: {tensor.shape[1]}")
            print(f"  Features: {tensor.shape[2]}")
            print(f"  Direct .pt file available at: {pt_file}")
            print(f"  Ready for Sequential VAE training")

        except FileNotFoundError:
            print(f"Error: Parquet file not found at {args.parquet_path}")
            exit(1)
        except Exception as e:
            print(f"Error during preprocessing: {str(e)}")
            import traceback
            traceback.print_exc()
            exit(1)

    args:
      - --parquet_path
      - {inputPath: parquet_path}
      - --num_features
      - {inputValue: num_features}
      - --seq_length
      - {inputValue: seq_length}
      - --preprocessed_path
      - {outputPath: preprocessed_path}
      - --metadata_path
      - {outputPath: metadata_path}
