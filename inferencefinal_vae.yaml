name: Run VAE Inference with Reconstruction Scoring
description: Loads trained VAE model, performs encoding/decoding for input data, and calculates reconstruction metrics
inputs:
  - name: trained_model
    type: Model
    description: Path to trained VAE model weights
  - name: model_config
    type: String
    description: VAE model configuration JSON string
  - name: test_data
    type: Dataset
    description: Path to test dataset for inference
  - name: inference_type
    type: String
    default: "reconstruction"
    description: Type of inference - reconstruction, generation, or encoding
  - name: num_samples
    type: Integer
    default: "100"
    description: Number of samples to process for inference
  - name: batch_size
    type: Integer
    default: "32"
    description: Batch size for inference processing
  - name: latent_dim
    type: Integer
    default: "64"
    description: Latent dimension size of the VAE model
  - name: generate_samples
    type: Integer
    default: "10"
    description: Number of new samples to generate (for generation mode)
outputs:
  - name: inference_results
    type: Data
    description: JSON file with inference results and reconstruction metrics
  - name: reconstructed_data
    type: Data
    description: Reconstructed data outputs from VAE decoder
  - name: latent_representations
    type: Data
    description: Encoded latent space representations
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        set -e
        pip install scikit-learn matplotlib seaborn
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, json, os, torch, pickle, numpy as np
        from sklearn.metrics import mean_squared_error, mean_absolute_error
        import torch.nn.functional as F
        import torch.nn as nn
        from torch.utils.data import DataLoader, Dataset

        # Define ProcessedVAEDataset class for unpickling
        class ProcessedVAEDataset(Dataset):
            def __init__(self, data, labels, vae_type='standard'):
                self.data = data
                self.labels = labels
                self.vae_type = vae_type
                self.class_mapping = {}
            
            def __len__(self):
                return len(self.data)
            
            def __getitem__(self, idx):
                return self.data[idx], self.labels[idx]

        def calculate_reconstruction_metrics(original, reconstructed):
            # Calculate various reconstruction quality metrics
            # Flatten arrays for metric calculation
            original_flat = original.flatten()
            reconstructed_flat = reconstructed.flatten()
            
            # MSE (Mean Squared Error)
            mse = mean_squared_error(original_flat, reconstructed_flat)
            
            # MAE (Mean Absolute Error) 
            mae = mean_absolute_error(original_flat, reconstructed_flat)
            
            # RMSE (Root Mean Squared Error)
            rmse = np.sqrt(mse)
            
            # PSNR (Peak Signal-to-Noise Ratio)
            if mse > 0:
                max_pixel = np.max(original_flat)
                psnr = 20 * np.log10(max_pixel / np.sqrt(mse))
            else:
                psnr = float('inf')
            
            # Structural Similarity (simplified)
            correlation = np.corrcoef(original_flat, reconstructed_flat)[0, 1]
            if np.isnan(correlation):
                correlation = 0.0
                
            return {
                'mse': float(mse),
                'mae': float(mae), 
                'rmse': float(rmse),
                'psnr': float(psnr),
                'correlation': float(correlation)
            }

        def vae_encode(model, x, device):
            # Encode input data to latent space
            model.eval()
            with torch.no_grad():
                x = x.to(device)
                if hasattr(model, 'encode'):
                    mu, logvar = model.encode(x)
                    return mu, logvar
                else:
                    # For models without explicit encode method
                    output = model(x)
                    if isinstance(output, tuple) and len(output) >= 3:
                        _, mu, logvar = output
                        return mu, logvar
                    else:
                        raise ValueError("Model doesn't support encoding operation")

        def vae_decode(model, z, device):
            # Decode latent representations to reconstructed data
            model.eval()
            with torch.no_grad():
                z = z.to(device)
                if hasattr(model, 'decode'):
                    return model.decode(z)
                else:
                    raise ValueError("Model doesn't support decoding operation")

        def vae_reconstruct(model, x, device):
            # Full VAE forward pass for reconstruction
            model.eval()
            with torch.no_grad():
                x = x.to(device)
                output = model(x)
                if isinstance(output, tuple):
                    reconstructed = output[0]  # First element is usually reconstruction
                    return reconstructed
                else:
                    return output

        def generate_new_samples(model, num_samples, latent_dim, device):
            # Generate new samples by sampling from latent space
            model.eval()
            with torch.no_grad():
                # Sample from standard normal distribution
                z = torch.randn(num_samples, latent_dim).to(device)
                generated = vae_decode(model, z, device)
                return generated

        parser = argparse.ArgumentParser()
        parser.add_argument("--trained_model", type=str, required=True)
        parser.add_argument("--model_config", type=str, required=True)
        parser.add_argument("--test_data", type=str, required=True)
        parser.add_argument("--inference_type", type=str, default="reconstruction")
        parser.add_argument("--num_samples", type=int, default=100)
        parser.add_argument("--batch_size", type=int, default=32)
        parser.add_argument("--latent_dim", type=int, default=64)
        parser.add_argument("--generate_samples", type=int, default=10)
        parser.add_argument("--inference_results", type=str, required=True)
        parser.add_argument("--reconstructed_data", type=str, required=True)
        parser.add_argument("--latent_representations", type=str, required=True)
        args = parser.parse_args()

        # Create output directories
        for output_path in [args.inference_results, args.reconstructed_data, args.latent_representations]:
            os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)

        device = "cuda" if torch.cuda.is_available() else "cpu"
        torch_device = torch.device(device)
        print(f"Using device: {device}")

        # Load model configuration
        print(f"Raw model_config: {args.model_config}")
        
        # args.model_config is always a string from command line
        try:
            # Try parsing as JSON string first
            config = json.loads(args.model_config)
            print(f"Successfully parsed config from JSON string: {config}")
        except (json.JSONDecodeError, ValueError) as e:
            print(f"Failed to parse as JSON string: {e}")
            # Try loading as file path
            try:
                if os.path.exists(args.model_config):
                    with open(args.model_config, "r", encoding="utf-8") as f:
                        config = json.load(f)
                    print(f"Successfully loaded config from file: {config}")
                else:
                    print(f"File not found: {args.model_config}, using default config")
                    config = {}
            except Exception as file_error:
                print(f"Failed to load config file: {file_error}, using default config")
                config = {}
        
        # Ensure config is a dictionary
        if not isinstance(config, dict):
            print(f"Warning: config is not a dict (type: {type(config)}), using empty dict")
            config = {}
        
        print(f"Final config: {config}")

        # Load test data
        with open(args.test_data, "rb") as f:
            test_data = pickle.load(f)
        
        print(f"Test data type: {type(test_data)}")
        
        # Handle different data formats
        if isinstance(test_data, DataLoader):
            print("Test data is a DataLoader, extracting tensors...")
            all_data = []
            all_labels = []
            for batch_data, batch_labels in test_data:
                all_data.append(batch_data)
                all_labels.append(batch_labels)
            test_tensor = torch.cat(all_data, dim=0)
            print(f"Extracted {len(test_tensor)} samples from DataLoader")
        elif isinstance(test_data, np.ndarray):
            test_tensor = torch.FloatTensor(test_data)
            print(f"Test data is numpy array with shape: {test_data.shape}")
        elif isinstance(test_data, torch.Tensor):
            test_tensor = test_data
            print(f"Test data is torch tensor with shape: {test_data.shape}")
        else:
            raise ValueError(f"Unsupported test data type: {type(test_data)}")
        
        print(f"Final test tensor shape: {test_tensor.shape}")

        # Define VAE model class (matching the trained model architecture)
        class VAE(nn.Module):
            def __init__(self, input_dim, hidden_dim, latent_dim):
                super(VAE, self).__init__()
                
                # Encoder
                self.encoder = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, hidden_dim // 2),
                    nn.ReLU()
                )
                
                # Latent space - use mu_layer and logvar_layer to match trained model
                self.mu_layer = nn.Linear(hidden_dim // 2, latent_dim)
                self.logvar_layer = nn.Linear(hidden_dim // 2, latent_dim)
                
                # Also create aliases for compatibility
                self.fc_mu = self.mu_layer
                self.fc_logvar = self.logvar_layer
                
                # Decoder
                self.decoder = nn.Sequential(
                    nn.Linear(latent_dim, hidden_dim // 2),
                    nn.ReLU(),
                    nn.Linear(hidden_dim // 2, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, input_dim),
                    nn.Sigmoid()
                )
            
            def encode(self, x):
                h = self.encoder(x)
                mu = self.mu_layer(h)
                logvar = self.logvar_layer(h)
                return mu, logvar
            
            def reparameterize(self, mu, logvar):
                std = torch.exp(0.5 * logvar)
                eps = torch.randn_like(std)
                return mu + eps * std
            
            def decode(self, z):
                return self.decoder(z)
            
            def forward(self, x):
                mu, logvar = self.encode(x)
                z = self.reparameterize(mu, logvar)
                recon_x = self.decode(z)
                return recon_x, mu, logvar

        # Load trained weights first to inspect architecture
        state_dict = torch.load(args.trained_model, map_location=torch_device)
        
        # Infer dimensions from state_dict if config is empty
        if not config or 'input_dim' not in config:
            print("Config is empty or missing dimensions, inferring from state_dict...")
            # Get dimensions from the state dict
            encoder_weight_shape = state_dict['encoder.0.weight'].shape
            hidden_dim = encoder_weight_shape[0]  # Output dim of first encoder layer
            input_dim = encoder_weight_shape[1]   # Input dim
            
            # Get latent dim from mu_layer or fc_mu
            if 'mu_layer.weight' in state_dict:
                latent_dim = state_dict['mu_layer.weight'].shape[0]
            elif 'fc_mu.weight' in state_dict:
                latent_dim = state_dict['fc_mu.weight'].shape[0]
            else:
                latent_dim = args.latent_dim
            
            print(f"Inferred dimensions - input_dim: {input_dim}, hidden_dim: {hidden_dim}, latent_dim: {latent_dim}")
        else:
            # Use config values
            input_dim = config.get('input_dim', 784)
            hidden_dim = config.get('hidden_dim', 256)
            latent_dim = config.get('latent_dim', args.latent_dim)
            print(f"Using config dimensions - input_dim: {input_dim}, hidden_dim: {hidden_dim}, latent_dim: {latent_dim}")
        
        # Create model with correct dimensions
        model = VAE(input_dim, hidden_dim, latent_dim)
        
        # Load the state dict with strict=False to ignore alias mismatches
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        if missing_keys:
            print(f"Missing keys (ignored): {missing_keys}")
        if unexpected_keys:
            print(f"Unexpected keys (ignored): {unexpected_keys}")
        
        model = model.to(torch_device)
        model.eval()

        print(f"Model loaded successfully and moved to {device}")

        # Limit samples if specified
        if args.num_samples > 0 and args.num_samples < len(test_tensor):
            test_tensor = test_tensor[:args.num_samples]
            print(f"Limited to {args.num_samples} samples")

        results = []
        all_reconstructed = []
        all_latent_mu = []
        all_latent_logvar = []

        print(f"Starting {args.inference_type} inference on {len(test_tensor)} samples")

        # Process data in batches
        for i in range(0, len(test_tensor), args.batch_size):
            batch = test_tensor[i:i+args.batch_size]
            
            if args.inference_type == "reconstruction":
                # Full reconstruction
                reconstructed = vae_reconstruct(model, batch, torch_device)
                
                # Calculate metrics for each sample in batch
                for j in range(len(batch)):
                    original = batch[j].cpu().numpy()
                    recon = reconstructed[j].cpu().numpy()
                    
                    metrics = calculate_reconstruction_metrics(original, recon)
                    
                    result = {
                        "sample_id": i + j,
                        "inference_type": "reconstruction",
                        "reconstruction_mse": metrics['mse'],
                        "reconstruction_mae": metrics['mae'], 
                        "reconstruction_rmse": metrics['rmse'],
                        "reconstruction_psnr": metrics['psnr'],
                        "reconstruction_correlation": metrics['correlation']
                    }
                    results.append(result)
                
                all_reconstructed.extend(reconstructed.cpu().numpy())
                
                # Also get latent representations
                mu, logvar = vae_encode(model, batch, torch_device)
                all_latent_mu.extend(mu.cpu().numpy())
                all_latent_logvar.extend(logvar.cpu().numpy())
                
            elif args.inference_type == "encoding":
                # Just encoding to latent space
                mu, logvar = vae_encode(model, batch, torch_device)
                
                for j in range(len(batch)):
                    result = {
                        "sample_id": i + j,
                        "inference_type": "encoding",
                        "latent_mu_mean": float(mu[j].mean().cpu()),
                        "latent_mu_std": float(mu[j].std().cpu()),
                        "latent_logvar_mean": float(logvar[j].mean().cpu()),
                        "latent_logvar_std": float(logvar[j].std().cpu())
                    }
                    results.append(result)
                
                all_latent_mu.extend(mu.cpu().numpy())
                all_latent_logvar.extend(logvar.cpu().numpy())
            
            print(f"Processed batch {i//args.batch_size + 1}/{(len(test_tensor)-1)//args.batch_size + 1}")

        # Generate new samples if requested
        if args.inference_type == "generation" or args.generate_samples > 0:
            print(f"Generating {args.generate_samples} new samples with latent_dim={latent_dim}")
            generated_samples = generate_new_samples(model, args.generate_samples, latent_dim, torch_device)
            
            for i in range(args.generate_samples):
                result = {
                    "sample_id": f"generated_{i}",
                    "inference_type": "generation",
                    "generated_sample_mean": float(generated_samples[i].mean().cpu()),
                    "generated_sample_std": float(generated_samples[i].std().cpu())
                }
                results.append(result)
            
            all_reconstructed.extend(generated_samples.cpu().numpy())

        # Calculate overall statistics
        if results:
            if args.inference_type == "reconstruction":
                avg_mse = np.mean([r['reconstruction_mse'] for r in results if 'reconstruction_mse' in r])
                avg_mae = np.mean([r['reconstruction_mae'] for r in results if 'reconstruction_mae' in r])
                avg_psnr = np.mean([r['reconstruction_psnr'] for r in results if 'reconstruction_psnr' in r and r['reconstruction_psnr'] != float('inf')])
                
                summary = {
                    "inference_summary": {
                        "total_samples": len([r for r in results if 'reconstruction_mse' in r]),
                        "average_mse": float(avg_mse),
                        "average_mae": float(avg_mae),
                        "average_psnr": float(avg_psnr),
                        "inference_type": args.inference_type
                    }
                }
                results.append(summary)

        # Save results
        with open(args.inference_results, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # Save reconstructed data
        if all_reconstructed:
            with open(args.reconstructed_data, "wb") as f:
                pickle.dump(np.array(all_reconstructed), f)

        # Save latent representations
        latent_data = {
            "mu": np.array(all_latent_mu) if all_latent_mu else None,
            "logvar": np.array(all_latent_logvar) if all_latent_logvar else None
        }
        with open(args.latent_representations, "wb") as f:
            pickle.dump(latent_data, f)

        print(f"Inference completed. Results saved to {args.inference_results}")
        print(f"Reconstructed data saved to {args.reconstructed_data}")
        print(f"Latent representations saved to {args.latent_representations}")

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --model_config
      - {inputValue: model_config}
      - --test_data
      - {inputPath: test_data}
      - --inference_type
      - {inputValue: inference_type}
      - --num_samples
      - {inputValue: num_samples}
      - --batch_size
      - {inputValue: batch_size}
      - --latent_dim
      - {inputValue: latent_dim}
      - --generate_samples
      - {inputValue: generate_samples}
      - --inference_results
      - {outputPath: inference_results}
      - --reconstructed_data
      - {outputPath: reconstructed_data}
      - --latent_representations
      - {outputPath: latent_representations}
