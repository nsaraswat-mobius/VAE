name: Failure Signature Dataset Loader 
description: Optimized dataset loader for failure signature data with enhanced synthetic data generation
inputs:
  - {name: api_url, type: String, description: 'API URL to fetch failure signature dataset'}
  - {name: access_token, type: string, description: 'Bearer access token for API auth'}
outputs:
  - {name: train_data, type: Dataset}
  - {name: val_data, type: Dataset}
  - {name: data_shape, type: Dataset}
  - {name: vae_type, type: Dataset}
  - {name: preprocessing_config, type: Dataset}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import json
        import argparse
        import pickle
        import numpy as np
        import requests
        import random
        import torch
        from torch.utils.data import DataLoader, TensorDataset

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--api_url', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--train_data', type=str, required=True)
            parser.add_argument('--val_data', type=str, required=True)
            parser.add_argument('--data_shape', type=str, required=True)
            parser.add_argument('--vae_type', type=str, required=True)
            parser.add_argument('--preprocessing_config', type=str, required=True)
            args = parser.parse_args()

            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            
            print("Loading failure signature dataset...")
            
            # Generate synthetic failure data for demo (optimized version)
            print("Using enhanced synthetic failure data for demo...")
            failure_logs = [
                "ERROR 1045: Access denied for user 'root'@'localhost'",
                "SQLException: Connection timeout after 30 seconds",
                "OutOfMemoryError: Java heap space exceeded 4GB limit", 
                "MemoryError: Unable to allocate array with 1000000 elements",
                "ConnectionError: Max retries exceeded with url /api/data",
                "SocketTimeoutException: Read timeout on socket connection",
                "AccessDenied: Permission denied for resource /secure/data",
                "NullPointerException at getUserById() method line 125",
                "TimeoutError: Request timed out after 60 seconds",
                "HTTPTimeoutError: Connection timeout to external service",
                "FileNotFoundError: Configuration file 'config.json' not found",
                "ValidationError: Invalid input parameters in request body",
                "AuthenticationError: Invalid API key or token expired",  
                "NetworkError: Unable to resolve hostname 'api.service.com'",
                "ConcurrencyError: Database deadlock detected on transaction",
                "ResourceExhaustedError: CPU usage exceeded 95% threshold",
                "SerializationError: Failed to parse JSON response data",
                "ProtocolError: HTTP/2 protocol violation in stream handling",
                "ConfigurationError: Missing required environment variables",
                "PermissionError: Write access denied to log directory"
            ] * 10  # 200 samples with enhanced variety
            
            # Create enhanced feature embeddings
            embeddings = []
            labels = []
            
            # Enhanced feature extraction function
            def extract_features(log_text):
                text = log_text.upper()
                return [
                    len(log_text),                           # Text length
                    len(log_text.split()),                   # Word count  
                    1 if "ERROR" in text else 0,             # Has ERROR
                    1 if "TIMEOUT" in text else 0,           # Has TIMEOUT
                    1 if "MEMORY" in text else 0,            # Has MEMORY
                    1 if "CONNECTION" in text else 0,        # Has CONNECTION
                    1 if "NULL" in text else 0,              # Has NULL
                    1 if "PERMISSION" in text else 0,        # Has PERMISSION
                    1 if "AUTHENTICATION" in text else 0,    # Has AUTH
                    1 if "NETWORK" in text else 0,           # Has NETWORK
                    1 if any(x in text for x in ["HTTP", "API"]) else 0,  # Protocol/API
                    1 if any(x in text for x in ["DATABASE", "SQL"]) else 0,  # Database
                ] + [random.random() for _ in range(500)]    # Random features to reach 512
            
            for i, log in enumerate(failure_logs):
                features = extract_features(log)
                embeddings.append(features)
                labels.append(i % 8)  # 8 distinct failure categories
            
            embeddings = np.array(embeddings)
            labels = np.array(labels)
            
            # Simple train/val split
            n_samples = len(embeddings)
            indices = np.random.permutation(n_samples)
            split_point = int(0.8 * n_samples)
            
            train_indices = indices[:split_point]
            val_indices = indices[split_point:]
            
            # Create PyTorch tensors
            train_embeddings = torch.FloatTensor(embeddings[train_indices])
            train_labels = torch.LongTensor(labels[train_indices])
            val_embeddings = torch.FloatTensor(embeddings[val_indices])
            val_labels = torch.LongTensor(labels[val_indices])
            
            # Create DataLoaders (expected by preprocessing component)
            train_dataset = TensorDataset(train_embeddings, train_labels)
            val_dataset = TensorDataset(val_embeddings, val_labels)
            train_data = DataLoader(train_dataset, batch_size=32, shuffle=True)
            val_data = DataLoader(val_dataset, batch_size=32, shuffle=False)
            
            # Create output directories
            os.makedirs(os.path.dirname(args.train_data), exist_ok=True)
            os.makedirs(os.path.dirname(args.val_data), exist_ok=True)
            os.makedirs(os.path.dirname(args.data_shape), exist_ok=True)
            os.makedirs(os.path.dirname(args.vae_type), exist_ok=True)
            os.makedirs(os.path.dirname(args.preprocessing_config), exist_ok=True)
            
            # Save outputs
            with open(args.train_data, 'wb') as f:
                pickle.dump(train_data, f)
            
            with open(args.val_data, 'wb') as f:
                pickle.dump(val_data, f)
            
            # Write string values without any quotes
            with open(args.data_shape, 'w') as f:
                f.write("512,1,1")
            
            with open(args.vae_type, 'w') as f:
                f.write("standard")
            
            # Write preprocessing config as JSON string
            preprocessing_config = {
                "input_dim": 512,
                "num_classes": 8,
                "dataset_type": "failure_signature"
            }
            
            with open(args.preprocessing_config, 'w') as f:
                json.dump(preprocessing_config, f)
            
            print(" Enhanced failure signature dataset loading complete!")
            print(f" Training samples: {len(train_data)}")
            print(f" Validation samples: {len(val_data)}")
            print(f" Feature dimension: 512 (enhanced)")
            print(f"  Failure categories: 8")  
            print(f" VAE type: standard")
            print(f" Ready for VAE pipeline processing!")

        if __name__ == '__main__':
            main()
    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --train_data
      - {outputPath: train_data}
      - --val_data
      - {outputPath: val_data}
      - --data_shape
      - {outputPath: data_shape}
      - --vae_type
      - {outputPath: vae_type}
      - --preprocessing_config
      - {outputPath: preprocessing_config}
