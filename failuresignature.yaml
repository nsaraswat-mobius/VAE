name: Failure Signature Dataset Loader Simple
description: Simple dataset loader that fetches failure signature data from API
inputs:
  - name: api_url
    type: String
    description: API URL to fetch failure signature dataset
  - name: access_token
    type: String
    description: Bearer access token for API auth
outputs:
  - name: train_data
    type: Dataset
    description: Training data pickle file
  - name: val_data
    type: Dataset
    description: Validation data pickle file
  - name: data_shape
    type: String
    description: Data shape
  - name: vae_type
    type: String
    description: VAE type
  - name: preprocessing_config
    type: String
    description: Preprocessing configuration JSON
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import json
        import argparse
        import pickle
        import numpy as np
        import requests
        import random

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--api_url', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--train_data', type=str, required=True)
            parser.add_argument('--val_data', type=str, required=True)
            parser.add_argument('--data_shape', type=str, required=True)
            parser.add_argument('--vae_type', type=str, required=True)
            parser.add_argument('--preprocessing_config', type=str, required=True)
            args = parser.parse_args()

            # Read access token
            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            
            print("Loading failure signature dataset...")
            
            # Try to fetch from API
            try:
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {access_token}"
                }
                payload = {
                    "dbType": "TIDB",
                    "entityId": "",
                    "entityIds": [],
                    "ownedOnly": False,
                    "projections": [],
                    "filter": {},
                    "startTime": 0,
                    "endTime": 0
                }
                
                response = requests.post(args.api_url, headers=headers, json=payload, timeout=30)
                response.raise_for_status()
                api_data = response.json()
                print(f"Successfully fetched data from API: {len(api_data) if isinstance(api_data, list) else 1} items")
                
                # Extract failure logs from API response
                failure_logs = []
                if isinstance(api_data, list):
                    for item in api_data:
                        if isinstance(item, dict):
                            for field in ['error_message', 'log_entry', 'failure_description', 'message', 'error']:
                                if field in item and item[field]:
                                    failure_logs.append(str(item[field]))
                                    break
                            else:
                                failure_logs.append(str(item))
                        else:
                            failure_logs.append(str(item))
                else:
                    failure_logs.append(str(api_data))
                    
            except Exception as e:
                print(f"API failed: {e}")
                print("Using synthetic failure data...")
                # Generate simple synthetic data
                failure_logs = [
                    "ERROR 1045: Access denied for user",
                    "SQLException: Connection timeout",
                    "OutOfMemoryError: Java heap space",
                    "MemoryError: Unable to allocate array",
                    "ConnectionError: Max retries exceeded",
                    "SocketTimeoutException: Read timeout",
                    "AccessDenied: Permission denied",
                    "NullPointerException at getUserById",
                    "TimeoutError: Request timed out",
                    "HTTPTimeoutError: Connection timeout"
                ] * 20  # 200 samples
            
            # Create simple embeddings (just use text length and random features)
            embeddings = []
            labels = []
            
            for i, log in enumerate(failure_logs):
                # Simple features: text length, word count, has "ERROR", has "timeout"
                features = [
                    len(log),
                    len(log.split()),
                    1 if "ERROR" in log.upper() else 0,
                    1 if "TIMEOUT" in log.upper() else 0,
                    1 if "MEMORY" in log.upper() else 0
                ] + [random.random() for _ in range(507)]  # Pad to 512 features
                
                embeddings.append(features)
                labels.append(i % 8)  # 8 failure types
            
            embeddings = np.array(embeddings)
            labels = np.array(labels)
            
            # Simple train/val split
            n_samples = len(embeddings)
            indices = np.random.permutation(n_samples)
            split_point = int(0.8 * n_samples)
            
            train_indices = indices[:split_point]
            val_indices = indices[split_point:]
            
            train_data = [(embeddings[i], labels[i]) for i in train_indices]
            val_data = [(embeddings[i], labels[i]) for i in val_indices]
            
            # Create output directories
            os.makedirs(os.path.dirname(args.train_data), exist_ok=True)
            os.makedirs(os.path.dirname(args.val_data), exist_ok=True)
            os.makedirs(os.path.dirname(args.data_shape), exist_ok=True)
            os.makedirs(os.path.dirname(args.vae_type), exist_ok=True)
            os.makedirs(os.path.dirname(args.preprocessing_config), exist_ok=True)
            
            # Save outputs
            with open(args.train_data, 'wb') as f:
                pickle.dump(train_data, f)
            
            with open(args.val_data, 'wb') as f:
                pickle.dump(val_data, f)
            
            with open(args.data_shape, 'w') as f:
                f.write("512,1,1")
            
            with open(args.vae_type, 'w') as f:
                f.write("standard")
            
            preprocessing_config = {
                "input_dim": 512,
                "num_classes": 8,
                "dataset_type": "failure_signature"
            }
            
            with open(args.preprocessing_config, 'w') as f:
                json.dump(preprocessing_config, f)
            
            print("Dataset loading complete!")
            print(f"Training samples: {len(train_data)}")
            print(f"Validation samples: {len(val_data)}")
            print(f"Data shape: 512,1,1")
            print(f"VAE type: standard")

        if __name__ == '__main__':
            main()
    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --train_data
      - {outputPath: train_data}
      - --val_data
      - {outputPath: val_data}
      - --data_shape
      - {outputPath: data_shape}
      - --vae_type
      - {outputPath: vae_type}
      - --preprocessing_config
      - {outputPath: preprocessing_config}
