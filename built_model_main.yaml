name: Initialize VAE Model
description: Initializes the VAE model for failure signature anomaly detection with dataset conversion.
inputs:
  - {name: config, type: String}
  - {name: input_dataset, type: Dataset}  # Add dataset input
outputs:
  - {name: model, type: Model}
  - {name: json_string, type: String}     # Add string output for Update Schema Row
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        import pandas as pd
        import torch
        
        from nesy_factory.VAE.standard_vae import StandardVAE
        from nesy_factory.VAE.beta_vae import BetaVAE
        from nesy_factory.VAE.conditional_vae import ConditionalVAE

        parser = argparse.ArgumentParser()
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--input_dataset', type=str, required=True)
        parser.add_argument('--model', type=str, required=True)
        parser.add_argument('--json_string', type=str, required=True)
        args = parser.parse_args()

        # Convert Dataset to JSON String for Update Schema Row
        try:
            # Read the dataset file
            print("Converting dataset to JSON string...")
            
            # Try different file formats
            try:
                df = pd.read_parquet(args.input_dataset)
                print("Successfully read dataset as Parquet")
            except:
                try:
                    df = pd.read_csv(args.input_dataset)
                    print("Successfully read dataset as CSV")
                except:
                    try:
                        df = pd.read_json(args.input_dataset)
                        print("Successfully read dataset as JSON")
                    except:
                        # Create minimal dataset if all formats fail
                        df = pd.DataFrame({'info': ['dataset_conversion']})
                        print("Created minimal dataset for conversion")
            
            # Convert to JSON string
            json_data = df.to_json(orient='records', indent=2)
            
            # Write JSON string output
            os.makedirs(os.path.dirname(args.json_string), exist_ok=True)
            with open(args.json_string, 'w') as f:
                f.write(json_data)
            
            print(f"Successfully converted dataset to JSON string")
            print(f"Dataset shape: {df.shape}")
            print(f"JSON output saved to: {args.json_string}")
            
        except Exception as e:
            print(f"Error converting dataset: {e}")
            # Create a default JSON string if conversion fails
            default_json = json.dumps({"status": "conversion_failed", "error": str(e)})
            with open(args.json_string, 'w') as f:
                f.write(default_json)
            print("Created fallback JSON string")

        # Continue with original VAE model initialization
        config = json.loads(args.config)

        # Get input dimension from config - use actual processed dimension
        if 'processed_input_dim' in config:
            # Use the actual dimension from preprocessing step
            input_dim = config['processed_input_dim']
            print(f"Using processed input dimension: {input_dim}")
        elif 'input_dim' in config:
            input_dim = config['input_dim']
            print(f"Using configured input dimension: {input_dim}")
        elif 'feature_columns' in config:
            # This is just an estimate - actual may be higher due to one-hot encoding
            base_features = len(config['feature_columns'])
            # Assume worst case: all categorical with 10 categories each
            input_dim = base_features * 10  # Conservative estimate
            print(f"Warning: Estimating input dimension as {input_dim} based on {base_features} features")
            print("Note: Actual dimension may differ due to one-hot encoding")
        else:
            # Default fallback - this should not happen in production
            input_dim = 50  # Increased default to handle typical one-hot encoded data
            print(f"Warning: input_dim not found in config, using default value of {input_dim}")
        
        print(f"Final input dimension: {input_dim}")

        # Build VAE model configuration
        model_config = {
            'input_dim': input_dim,
            'latent_dim': config.get('latent_dim', 16),
            'hidden_dim': config.get('hidden_dim', 64),
            'num_layers': config.get('num_layers', 2),
            'dropout': config.get('dropout', 0.1),
            'beta': config.get('beta', 1.0),  # For Beta-VAE
            'optimizer': 'adam',
            'learning_rate': config.get('learning_rate', 0.001),
            'epochs': config.get('epochs', 100),
            'loss_function': config.get('loss_function', 'ELBO (reconstruction MSE + KL divergence)')
        }

        print(f"Model configuration: {model_config}")

        # Initialize the appropriate VAE model based on model_type
        model_type = config.get('model_type', 'VAE')
        
        if model_type == 'StandardVAE' or model_type == 'VAE':
            model_obj = StandardVAE(model_config)
            print("Initialized Standard VAE")
        elif model_type == 'BetaVAE':
            model_obj = BetaVAE(model_config)
            print(f"Initialized Beta-VAE with beta={model_config['beta']}")
        elif model_type == 'ConditionalVAE':
            # For conditional VAE, we might need additional configuration
            num_classes = config.get('num_classes', 10)
            model_config['num_classes'] = num_classes
            model_obj = ConditionalVAE(model_config)
            print(f"Initialized Conditional VAE with {num_classes} classes")
        else:
            raise ValueError(f"Invalid VAE model type specified: {model_type}")

        # Save the model
        os.makedirs(os.path.dirname(args.model), exist_ok=True)
        with open(args.model, "wb") as f:
            pickle.dump(model_obj, f)

        print(f"Saved {model_type} model to {args.model}")
        print(f"Model parameters: {sum(p.numel() for p in model_obj.parameters() if p.requires_grad)} trainable parameters")
    args:
      - --config
      - {inputValue: config}
      - --input_dataset
      - {inputPath: input_dataset}
      - --model
      - {outputPath: model}
      - --json_string
      - {outputPath: json_string}
