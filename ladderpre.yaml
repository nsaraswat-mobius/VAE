name: preprocess_worldbank_tensor_ladder
description: Preprocesses World Bank parquet data for Ladder VAE - handles Inf values, normalizes, outputs 2D torch tensor (no sequence reshaping)
inputs:
  - {name: parquet_path, type: String, description: "Path to tensor_values.parquet"}
  - {name: num_features, type: Integer, description: "Number of features to use (default 16197)", default: "16197"}
outputs:
  - {name: preprocessed_path, type: String, description: "Path to saved preprocessed 2D tensor (.pt file)"}
  - {name: metadata_path, type: String, description: "Path to metadata JSON (shape, features, etc)"}

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet pyarrow pandas numpy torch || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet pyarrow pandas numpy torch --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pyarrow.parquet as pq
        import pandas as pd
        import numpy as np
        import torch
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--parquet_path', type=str, required=True)
        parser.add_argument('--num_features', type=int, default=16197)
        parser.add_argument('--preprocessed_path', type=str, required=True)
        parser.add_argument('--metadata_path', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 70)
        print("PREPROCESSING WORLD BANK TENSOR FOR LADDER VAE")
        print("=" * 70)
        
        try:
            # Step 1: Load parquet
            print("\\n[1/3] Loading parquet file...")
            
            # Handle Kubeflow artifact directory structure
            parquet_path = args.parquet_path
            if os.path.isdir(parquet_path):
                # Find .parquet file in directory
                files = [f for f in os.listdir(parquet_path) if f.endswith('.parquet')]
                if files:
                    parquet_path = os.path.join(parquet_path, files[0])
                    print(f"   Found parquet file: {parquet_path}")
                else:
                    raise FileNotFoundError(f"No .parquet files found in {parquet_path}")
            
            print(f"   Reading from: {parquet_path}")
            pf = pq.ParquetFile(parquet_path)
            df = pf.read().to_pandas()
            print(f"   Loaded: {df.shape[0]} rows x {df.shape[1]} columns")
            
            # Get feature columns
            feature_cols = [c for c in df.columns if c.startswith('I')][:args.num_features]
            print(f"   Using {len(feature_cols)} features")
            
            # Step 2: Handle Inf/NaN values
            print("\\n[2/3] Handling Inf/NaN values...")
            data = df[feature_cols].values.astype(np.float32)
            inf_count = (~np.isfinite(data)).sum()
            data[~np.isfinite(data)] = 0.0
            print(f"   Replaced {inf_count} Inf/NaN values with 0")
            print(f"   Data shape: {data.shape}")
            
            # Step 3: Normalize (z-score)
            print("\\n[3/3] Normalizing features (z-score)...")
            mean_vals = np.mean(data, axis=0, keepdims=True)
            std_vals = np.std(data, axis=0, keepdims=True)
            std_vals[std_vals == 0] = 1.0  # Avoid division by zero
            data_normalized = (data - mean_vals) / std_vals
            print(f"   Mean: [{mean_vals.min():.4f}, {mean_vals.max():.4f}]")
            print(f"   Std:  [{std_vals.min():.4f}, {std_vals.max():.4f}]")
            print(f"   Normalized shape: {data_normalized.shape}")
            
            # Save as torch tensor (2D - no sequence reshaping for Ladder VAE)
            print("\\n[SAVING] Saving preprocessed 2D tensor...")
            
            # Ensure Kubeflow output directories exist
            os.makedirs(args.preprocessed_path, exist_ok=True)
            os.makedirs(args.metadata_path, exist_ok=True)
            
            tensor = torch.FloatTensor(data_normalized)
            output_file = os.path.join(args.preprocessed_path, "tensor_preprocessed_ladder.pt")
            torch.save(tensor, output_file)
            print(f"   Saved to: {output_file}")
            print(f"   Shape: {tensor.shape} (samples, features) â† 2D for Ladder VAE")
            file_size_gb = os.path.getsize(output_file) / 1e9
            print(f"   File size: {file_size_gb:.2f} GB")
            
            if file_size_gb > 2.0:
                print(f"    WARNING: Large file detected ({file_size_gb:.2f} GB)")
                print(f"   Consider:")
                print(f"     - Using fewer features (--num_features parameter)")
                print(f"     - Using cloud storage (GCS, S3) instead of local")
            
            # Write output path to file for Kubeflow
            with open(os.path.join(args.preprocessed_path, "data"), "w") as f:
                f.write(output_file)
            
            # Save metadata
            metadata = {
                "shape": list(tensor.shape),
                "num_samples": tensor.shape[0],
                "num_features": tensor.shape[1],
                "features_used": len(feature_cols),
                "inf_values_replaced": int(inf_count),
                "normalization": "z-score (mean=0, std=1)",
                "output_file": output_file,
                "model_type": "ladder_vae",
                "tensor_format": "2D (no temporal sequences)"
            }
            
            metadata_file = os.path.join(args.metadata_path, "metadata.json")
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"   Metadata saved: {metadata_file}")
            
            # Write metadata path to file for Kubeflow
            with open(os.path.join(args.metadata_path, "data"), "w") as f:
                f.write(metadata_file)
            
            print("\\n" + "=" * 70)
            print("PREPROCESSING COMPLETE!")
            print("=" * 70)
            print(f"\\nOutput Summary:")
            print(f"  Tensor shape: {tuple(tensor.shape)}")
            print(f"  Samples: {tensor.shape[0]}")
            print(f"  Features: {tensor.shape[1]}")
            print(f"  Format: 2D (batch_size, features)")
            print(f"  Ready for Ladder VAE training")
            print(f"\\nDifference from Sequential VAE:")
            print(f"  Sequential: Reshapes into (3053, 25, 16197) for temporal sequences")
            print(f"  Ladder:     Keeps as (76340, 16197) for static features")
            
        except FileNotFoundError:
            print(f"Error: Parquet file not found at {args.parquet_path}")
            exit(1)
        except Exception as e:
            print(f"Error during preprocessing: {str(e)}")
            import traceback
            traceback.print_exc()
            exit(1)
    
    args:
      - --parquet_path
      - {inputPath: parquet_path}
      - --num_features
      - {inputValue: num_features}
      - --preprocessed_path
      - {outputPath: preprocessed_path}
      - --metadata_path
      - {outputPath: metadata_path}
