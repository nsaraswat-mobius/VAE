name: Preprocess Failure Signature Data
description: Processes failure signature data and creates training and testing dataloaders for VAE.
inputs:
  - {name: json_data, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: train_loader, type: Dataset}
  - {name: test_loader, type: Dataset}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import numpy as np
        import pandas as pd
        import torch
        from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
        from torch.utils.data import TensorDataset, DataLoader

        def extract_failure_signatures(json_data, config):
            # Extract features from performance/metrics data for VAE
            signatures = []
            encoding_strategy = config.get('encoding_strategy', 'single_feature')
            
            print(f"Using encoding strategy: {encoding_strategy}")
            
            if encoding_strategy == 'single_feature':
                # Single feature extraction
                signature_field = config.get('signature_field', 'value')
                print(f"Extracting single feature: {signature_field}")
                
                for item in json_data:
                    if signature_field in item:
                        try:
                            signature = [float(item[signature_field])]
                            signatures.append(signature)
                        except (ValueError, TypeError):
                            continue
                            
            elif encoding_strategy == 'multi_feature':
                # Multi-feature extraction
                numerical_features = config.get('numerical_features', ['value'])
                categorical_features = config.get('categorical_features', [])
                
                print(f"Extracting numerical features: {numerical_features}")
                print(f"Extracting categorical features: {categorical_features}")
                
                # First pass: collect all categorical values for encoding
                categorical_mappings = {}
                for feature in categorical_features:
                    unique_values = set()
                    for item in json_data:
                        if feature in item:
                            unique_values.add(str(item[feature]))
                    categorical_mappings[feature] = {val: idx for idx, val in enumerate(sorted(unique_values))}
                
                # Second pass: extract features
                for item in json_data:
                    signature = []
                    valid_item = True
                    
                    # Add numerical features
                    for feature in numerical_features:
                        if feature in item:
                            try:
                                value = float(item[feature])
                                signature.append(value)
                            except (ValueError, TypeError):
                                valid_item = False
                                break
                        else:
                            valid_item = False
                            break
                    
                    # Add categorical features (one-hot encoded)
                    if valid_item:
                        for feature in categorical_features:
                            if feature in item:
                                cat_value = str(item[feature])
                                if cat_value in categorical_mappings[feature]:
                                    # One-hot encode: create vector of zeros with 1 at correct position
                                    one_hot = [0.0] * len(categorical_mappings[feature])
                                    one_hot[categorical_mappings[feature][cat_value]] = 1.0
                                    signature.extend(one_hot)
                                else:
                                    valid_item = False
                                    break
                            else:
                                valid_item = False
                                break
                    
                    if valid_item and len(signature) > 0:
                        signatures.append(signature)
                        
            if len(signatures) == 0:
                raise ValueError("No valid signatures found")
                
            return np.array(signatures, dtype=np.float32)

        def preprocess_for_vae(signatures, config):
            # Apply preprocessing for VAE training
            print(f"Input signature shape: {signatures.shape}")
            
            # Handle missing/invalid values
            signatures = np.nan_to_num(signatures, nan=0.0)
            signatures = np.clip(signatures, -1e6, 1e6)
            
            # Apply scaling
            scaler_type = config.get('scaler_type', 'standard')
            if scaler_type == 'minmax':
                scaler = MinMaxScaler()
            elif scaler_type == 'robust':
                scaler = RobustScaler()
            else:
                scaler = StandardScaler()
            
            signatures_scaled = scaler.fit_transform(signatures)
            print(f"Applied {scaler_type} scaling")
            
            return signatures_scaled

        parser = argparse.ArgumentParser()
        parser.add_argument('--json_data', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--train_loader', type=str, required=True)
        parser.add_argument('--test_loader', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)

        with open(args.json_data, 'rb') as f:
            json_data = pickle.load(f)
        print(f"Loaded failure signature data. Records: {len(json_data)}")

        # Extract failure signatures
        signatures = extract_failure_signatures(json_data, config)
        print(f"Extracted {len(signatures)} signatures with dimension {signatures.shape[1]}")

        # Deduplicate signatures if requested
        if config.get('deduplicate', False):
            from sklearn.metrics.pairwise import cosine_similarity
            threshold = config.get('dedup_threshold', 0.95)
            
            print(f"Deduplicating with threshold: {threshold}")
            similarity_matrix = cosine_similarity(signatures)
            
            to_remove = set()
            for i in range(len(signatures)):
                if i in to_remove:
                    continue
                for j in range(i + 1, len(signatures)):
                    if j in to_remove:
                        continue
                    if similarity_matrix[i, j] > threshold:
                        to_remove.add(j)
            
            keep_indices = [i for i in range(len(signatures)) if i not in to_remove]
            signatures = signatures[keep_indices]
            
            print(f"Removed {len(to_remove)} duplicate signatures, kept {len(signatures)}")

        # Preprocess for VAE
        signatures_processed = preprocess_for_vae(signatures, config)

        # Create train/test split
        np.random.seed(config.get('random_seed', 42))
        indices = np.random.permutation(len(signatures_processed))
        signatures_shuffled = signatures_processed[indices]
        
        train_ratio = config.get('train_ratio', 0.8)
        train_size = int(len(signatures_shuffled) * train_ratio)
        
        X_train = signatures_shuffled[:train_size]
        X_test = signatures_shuffled[train_size:]
        print(f"Train data shape: {X_train.shape}, Test data shape: {X_test.shape}")

        # Convert to PyTorch tensors
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)

        # For VAE, input and target are the same (reconstruction)
        train_dataset = TensorDataset(X_train_tensor, X_train_tensor)
        test_dataset = TensorDataset(X_test_tensor, X_test_tensor)

        # Create DataLoaders
        batch_size = config.get('batch_size', 64)
        train_loader_obj = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader_obj = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        # Save data loaders
        os.makedirs(os.path.dirname(args.train_loader), exist_ok=True)
        with open(args.train_loader, "wb") as f:
            pickle.dump(train_loader_obj, f)

        os.makedirs(os.path.dirname(args.test_loader), exist_ok=True)
        with open(args.test_loader, "wb") as f:
            pickle.dump(test_loader_obj, f)

        print(f"Saved train_loader to {args.train_loader}")
        print(f"Saved test_loader to {args.test_loader}")
        print(f"Train batches: {len(train_loader_obj)}")
        print(f"Test batches: {len(test_loader_obj)}")
    args:
      - --json_data
      - {inputPath: json_data}
      - --config
      - {inputValue: config}
      - --train_loader
      - {outputPath: train_loader}
      - --test_loader
      - {outputPath: test_loader}
