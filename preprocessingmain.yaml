name: Preprocess Failure Signature Data
description: Processes failure signature data and creates training and testing dataloaders for VAE.
inputs:
    - {name: json_data, type: Dataset}
    - {name: config, type: String}
outputs:
  - {name: train_loader, type: Dataset}
  - {name: test_loader, type: Dataset}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import numpy as np
        import pandas as pd
        import torch
        from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
        from torch.utils.data import TensorDataset, DataLoader

        def extract_failure_signatures(json_data, config):
            # Extract features from performance/metrics data for VAE
            signatures = []
            encoding_strategy = config.get('encoding_strategy', 'single_feature')
            
            print(f"Using encoding strategy: {encoding_strategy}")
            
            if encoding_strategy == 'single_feature':
                # Single feature extraction
                signature_field = config.get('signature_field', 'value')
                print(f"Extracting single feature: {signature_field}")
                
                for item in json_data:
                    if signature_field in item:
                        try:
                            signature = [float(item[signature_field])]
                            signatures.append(signature)
                        except (ValueError, TypeError):
                            continue
                            
            elif encoding_strategy == 'multi_feature':
                # Multi-feature extraction
                # Check if we have feature_columns (like RNN config) or separate numerical/categorical
                if 'feature_columns' in config:
                    all_features = config['feature_columns']
                    # Try to infer which are numerical vs categorical
                    numerical_features = []
                    categorical_features = []
                    
                    for feature in all_features:
                        # Sample first item to determine type
                        sample_found = False
                        for item in json_data:
                            if feature in item:
                                try:
                                    float(item[feature])
                                    numerical_features.append(feature)
                                    sample_found = True
                                    break
                                except (ValueError, TypeError):
                                    categorical_features.append(feature)
                                    sample_found = True
                                    break
                        if not sample_found:
                            print(f"Warning: Feature {feature} not found in data")
                else:
                    numerical_features = config.get('numerical_features', ['value'])
                    categorical_features = config.get('categorical_features', [])
                
                print(f"Extracting numerical features: {numerical_features}")
                print(f"Extracting categorical features: {categorical_features}")
                
                # First pass: collect all categorical values for encoding
                categorical_mappings = {}
                for feature in categorical_features:
                    unique_values = set()
                    for item in json_data:
                        if feature in item:
                            unique_values.add(str(item[feature]))
                    categorical_mappings[feature] = {val: idx for idx, val in enumerate(sorted(unique_values))}
                
                # Second pass: extract features
                for item in json_data:
                    signature = []
                    valid_item = True
                    
                    # Add numerical features
                    for feature in numerical_features:
                        if feature in item:
                            try:
                                value = float(item[feature])
                                signature.append(value)
                            except (ValueError, TypeError):
                                valid_item = False
                                break
                        else:
                            valid_item = False
                            break
                    
                    # Add categorical features (one-hot encoded)
                    if valid_item:
                        for feature in categorical_features:
                            if feature in item:
                                cat_value = str(item[feature])
                                if cat_value in categorical_mappings[feature]:
                                    # One-hot encode: create vector of zeros with 1 at correct position
                                    one_hot = [0.0] * len(categorical_mappings[feature])
                                    one_hot[categorical_mappings[feature][cat_value]] = 1.0
                                    signature.extend(one_hot)
                                else:
                                    valid_item = False
                                    break
                            else:
                                valid_item = False
                                break
                    
                    if valid_item and len(signature) > 0:
                        signatures.append(signature)
                        
            if len(signatures) == 0:
                raise ValueError("No valid signatures found")
                
            return np.array(signatures, dtype=np.float32)

        def preprocess_for_vae(signatures, config):
            # Apply preprocessing for VAE training
            print(f"Input signature shape: {signatures.shape}")
            
            # Handle missing/invalid values
            signatures = np.nan_to_num(signatures, nan=0.0)
            signatures = np.clip(signatures, -1e6, 1e6)
            
            # Apply scaling
            scaler_type = config.get('scaler_type', 'standard')
            if scaler_type == 'minmax':
                scaler = MinMaxScaler()
            elif scaler_type == 'robust':
                scaler = RobustScaler()
            else:
                scaler = StandardScaler()
            
            signatures_scaled = scaler.fit_transform(signatures)
            print(f"Applied {scaler_type} scaling")
            
            return signatures_scaled

        parser = argparse.ArgumentParser()
        parser.add_argument('--json_data', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--train_loader', type=str, required=True)
        parser.add_argument('--test_loader', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)

        # Handle different data input formats
        json_data = None
        data_path = args.json_data
        
        print(f"Loading data from: {data_path}")
        
        try:
            # Try to load as pickled data first
            with open(data_path, 'rb') as f:
                json_data = pickle.load(f)
            print(f"Loaded pickled data. Records: {len(json_data)}")
        except (pickle.UnpicklingError, EOFError, UnicodeDecodeError):
            try:
                # If pickle fails, try JSON
                import json as json_module
                with open(data_path, 'r', encoding='utf-8') as f:
                    json_data = json_module.load(f)
                print(f"Loaded JSON data. Records: {len(json_data)}")
            except (json.JSONDecodeError, UnicodeDecodeError):
                try:
                    # If JSON fails, try CSV
                    df = pd.read_csv(data_path)
                    json_data = df.to_dict('records')
                    print(f"Loaded CSV data. Records: {len(json_data)}")
                except Exception as e:
                    raise ValueError(f"Could not load data from {data_path}. Tried pickle, JSON, and CSV formats. Error: {e}")
        
        if json_data is None or len(json_data) == 0:
            raise ValueError("No data loaded or empty dataset")

        # Extract failure signatures
        signatures = extract_failure_signatures(json_data, config)
        print(f"Extracted {len(signatures)} signatures with dimension {signatures.shape[1]}")

        # Deduplicate signatures if requested
        if config.get('deduplicate', False):
            from sklearn.metrics.pairwise import cosine_similarity
            threshold = config.get('dedup_threshold', 0.95)
            
            print(f"Deduplicating with threshold: {threshold}")
            similarity_matrix = cosine_similarity(signatures)
            
            to_remove = set()
            for i in range(len(signatures)):
                if i in to_remove:
                    continue
                for j in range(i + 1, len(signatures)):
                    if j in to_remove:
                        continue
                    if similarity_matrix[i, j] > threshold:
                        to_remove.add(j)
            
            keep_indices = [i for i in range(len(signatures)) if i not in to_remove]
            signatures = signatures[keep_indices]
            
            print(f"Removed {len(to_remove)} duplicate signatures, kept {len(signatures)}")

        # Preprocess for VAE
        signatures_processed = preprocess_for_vae(signatures, config)

        # Create train/test split
        np.random.seed(config.get('random_seed', 42))
        indices = np.random.permutation(len(signatures_processed))
        signatures_shuffled = signatures_processed[indices]
        
        train_ratio = config.get('train_ratio', 0.8)
        train_size = int(len(signatures_shuffled) * train_ratio)
        
        X_train = signatures_shuffled[:train_size]
        X_test = signatures_shuffled[train_size:]
        print(f"Train data shape: {X_train.shape}, Test data shape: {X_test.shape}")

        # Convert to PyTorch tensors
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)

        # For VAE, input and target are the same (reconstruction)
        train_dataset = TensorDataset(X_train_tensor, X_train_tensor)
        test_dataset = TensorDataset(X_test_tensor, X_test_tensor)

        # Create DataLoaders
        batch_size = config.get('batch_size', 64)
        train_loader_obj = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader_obj = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        # Save data loaders
        os.makedirs(os.path.dirname(args.train_loader), exist_ok=True)
        with open(args.train_loader, "wb") as f:
            pickle.dump(train_loader_obj, f)

        os.makedirs(os.path.dirname(args.test_loader), exist_ok=True)
        with open(args.test_loader, "wb") as f:
            pickle.dump(test_loader_obj, f)

        print(f"Saved train_loader to {args.train_loader}")
        print(f"Saved test_loader to {args.test_loader}")
        print(f"Train batches: {len(train_loader_obj)}")
        print(f"Test batches: {len(test_loader_obj)}")
        args:
            - --json_data
            - {inputPath: json_data}
            - --config
            - {inputValue: config}
      - --train_loader
      - {outputPath: train_loader}
      - --test_loader
      - {outputPath: test_loader}
