name: Preprocess Failure Signature Data
description: Processes failure signature data and creates training and testing dataloaders for VAE.
inputs:
  - {name: json_data, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: train_loader, type: Dataset}
  - {name: test_loader, type: Dataset}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import numpy as np
        import pandas as pd
        import torch
        from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
        from torch.utils.data import TensorDataset, DataLoader

        def extract_failure_signatures(json_data, config):
            # Extract features from performance/metrics data for VAE
            signatures = []
            signature_field = config.get('signature_field', 'value')
            
            for item in json_data:
                if signature_field in item:
                    try:
                        # For VAE, we typically work with single values or simple feature vectors
                        signature = [float(item[signature_field])]
                        signatures.append(signature)
                    except (ValueError, TypeError):
                        continue
                        
            if len(signatures) == 0:
                raise ValueError("No valid signatures found")
                
            return np.array(signatures, dtype=np.float32)

        def preprocess_for_vae(signatures, config):
            # Apply preprocessing for VAE training
            print(f"Input signature shape: {signatures.shape}")
            
            # Handle missing/invalid values
            signatures = np.nan_to_num(signatures, nan=0.0)
            signatures = np.clip(signatures, -1e6, 1e6)
            
            # Apply scaling
            scaler_type = config.get('scaler_type', 'standard')
            if scaler_type == 'minmax':
                scaler = MinMaxScaler()
            elif scaler_type == 'robust':
                scaler = RobustScaler()
            else:
                scaler = StandardScaler()
            
            signatures_scaled = scaler.fit_transform(signatures)
            print(f"Applied {scaler_type} scaling")
            
            return signatures_scaled

        parser = argparse.ArgumentParser()
        parser.add_argument('--json_data', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--train_loader', type=str, required=True)
        parser.add_argument('--test_loader', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)

        with open(args.json_data, 'rb') as f:
            json_data = pickle.load(f)
        print(f"Loaded failure signature data. Records: {len(json_data)}")

        # Extract failure signatures
        signatures = extract_failure_signatures(json_data, config)
        print(f"Extracted {len(signatures)} signatures with dimension {signatures.shape[1]}")

        # Preprocess for VAE
        signatures_processed = preprocess_for_vae(signatures, config)

        # Create train/test split
        np.random.seed(config.get('random_seed', 42))
        indices = np.random.permutation(len(signatures_processed))
        signatures_shuffled = signatures_processed[indices]
        
        train_ratio = config.get('train_ratio', 0.8)
        train_size = int(len(signatures_shuffled) * train_ratio)
        
        X_train = signatures_shuffled[:train_size]
        X_test = signatures_shuffled[train_size:]
        print(f"Train data shape: {X_train.shape}, Test data shape: {X_test.shape}")

        # Convert to PyTorch tensors
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)

        # For VAE, input and target are the same (reconstruction)
        train_dataset = TensorDataset(X_train_tensor, X_train_tensor)
        test_dataset = TensorDataset(X_test_tensor, X_test_tensor)

        # Create DataLoaders
        batch_size = config.get('batch_size', 64)
        train_loader_obj = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader_obj = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        # Save data loaders
        os.makedirs(os.path.dirname(args.train_loader), exist_ok=True)
        with open(args.train_loader, "wb") as f:
            pickle.dump(train_loader_obj, f)

        os.makedirs(os.path.dirname(args.test_loader), exist_ok=True)
        with open(args.test_loader, "wb") as f:
            pickle.dump(test_loader_obj, f)

        print(f"Saved train_loader to {args.train_loader}")
        print(f"Saved test_loader to {args.test_loader}")
        print(f"Train batches: {len(train_loader_obj)}")
        print(f"Test batches: {len(test_loader_obj)}")
    args:
      - --json_data
      - {inputPath: json_data}
      - --config
      - {inputValue: config}
      - --train_loader
      - {outputPath: train_loader}
      - --test_loader
      - {outputPath: test_loader}
