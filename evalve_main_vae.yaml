name: Evaluate VAE Model
description: Evaluates the trained VAE model for failure signature anomaly detection.
inputs:
  - {name: trained_model, type: Model}
  - {name: test_loader, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: eval_metrics, type: Metrics}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        import numpy as np
        import torch
        import torch.nn as nn
        from sklearn.metrics import roc_auc_score, precision_recall_curve, auc
        
        from nesy_factory.VAE.standard_vae import StandardVAE
        from nesy_factory.VAE.beta_vae import BetaVAE
        from nesy_factory.VAE.conditional_vae import ConditionalVAE

        def calculate_reconstruction_error(model, data_loader, device):
            # Calculate reconstruction error for anomaly detection
            model.eval()
            reconstruction_errors = []
            
            with torch.no_grad():
                for batch_data in data_loader:
                    # Handle batch format
                    if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 1:
                        data = batch_data[0]
                    else:
                        data = batch_data
                    
                    data = data.to(device)
                    
                    # Forward pass
                    vae_output = model(data)
                    
                    # Extract reconstruction from VAE output
                    if isinstance(vae_output, dict):
                        possible_recon_keys = ['reconstruction', 'recon', 'x_recon', 'decoded', 'output']
                        recon_data = None
                        for key in possible_recon_keys:
                            if key in vae_output:
                                recon_data = vae_output[key]
                                break
                        if recon_data is None:
                            recon_data = list(vae_output.values())[0]
                    elif isinstance(vae_output, tuple):
                        recon_data = vae_output[0]  # First element is reconstruction
                    else:
                        recon_data = vae_output
                    
                    # Calculate reconstruction error per sample
                    mse_per_sample = torch.mean((recon_data - data)**2, dim=1)
                    reconstruction_errors.extend(mse_per_sample.cpu().numpy())
            
            return np.array(reconstruction_errors)

        def calculate_vae_losses(model, data_loader, device, beta=1.0):
            # Calculate VAE-specific losses (ELBO, reconstruction, KL)
            model.eval()
            total_elbo = 0
            total_recon = 0
            total_kl = 0
            num_samples = 0
            
            with torch.no_grad():
                for batch_data in data_loader:
                    # Handle batch format
                    if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 1:
                        data = batch_data[0]
                    else:
                        data = batch_data
                    
                    data = data.to(device)
                    vae_output = model(data)
                    
                    # Extract components
                    if isinstance(vae_output, dict):
                        possible_recon_keys = ['reconstruction', 'recon', 'x_recon', 'decoded', 'output']
                        possible_mu_keys = ['mu', 'mean', 'latent_mean', 'z_mean']
                        possible_logvar_keys = ['logvar', 'log_var', 'latent_logvar', 'z_logvar']
                        
                        recon_data = None
                        for key in possible_recon_keys:
                            if key in vae_output:
                                recon_data = vae_output[key]
                                break
                        
                        mu = None
                        for key in possible_mu_keys:
                            if key in vae_output:
                                mu = vae_output[key]
                                break
                        
                        logvar = None
                        for key in possible_logvar_keys:
                            if key in vae_output:
                                logvar = vae_output[key]
                                break
                        
                        # Default values if not found
                        if recon_data is None:
                            recon_data = list(vae_output.values())[0]
                        if mu is None:
                            mu = torch.zeros(data.size(0), getattr(model, 'latent_dim', 16)).to(device)
                        if logvar is None:
                            logvar = torch.zeros(data.size(0), getattr(model, 'latent_dim', 16)).to(device)
                            
                    elif isinstance(vae_output, tuple) and len(vae_output) >= 3:
                        recon_data, mu, logvar = vae_output[0], vae_output[1], vae_output[2]
                    else:
                        # Fallback: use encode/decode methods
                        if hasattr(model, 'encode') and hasattr(model, 'decode'):
                            mu, logvar = model.encode(data)
                            z = model.reparameterize(mu, logvar)
                            recon_data = model.decode(z)
                        else:
                            continue  # Skip this batch
                    
                    # Calculate losses
                    recon_loss = nn.MSELoss()(recon_data, data)
                    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / data.size(0)
                    elbo_loss = recon_loss + beta * kl_loss
                    
                    total_elbo += elbo_loss.item() * data.size(0)
                    total_recon += recon_loss.item() * data.size(0)
                    total_kl += kl_loss.item() * data.size(0)
                    num_samples += data.size(0)
            
            return {
                'avg_elbo_loss': total_elbo / num_samples,
                'avg_reconstruction_loss': total_recon / num_samples,
                'avg_kl_divergence': total_kl / num_samples
            }

        def detect_anomalies(reconstruction_errors, threshold_percentile=95):
            # Detect anomalies based on reconstruction error threshold
            threshold = np.percentile(reconstruction_errors, threshold_percentile)
            anomalies = reconstruction_errors > threshold
            
            return {
                'threshold': float(threshold),
                'num_anomalies': int(np.sum(anomalies)),
                'anomaly_rate': float(np.mean(anomalies)),
                'anomaly_indices': np.where(anomalies)[0].tolist()
            }

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--test_loader', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--eval_metrics', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)

        # Load test data (handle new format with metadata)
        with open(args.test_loader, 'rb') as f:
            test_data = pickle.load(f)
            
        if isinstance(test_data, dict) and 'loader' in test_data:
            test_loader_obj = test_data['loader']
            metadata = test_data['metadata']
            print(f"Loaded test data with {metadata['test_samples']} samples")
        else:
            test_loader_obj = test_data
            metadata = None
            print("Loaded test data (legacy format)")

        # Load trained model
        with open(args.trained_model, 'rb') as f:
            model_obj = pickle.load(f)

        # Set device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model_obj = model_obj.to(device)

        print("--- Starting VAE Model Evaluation ---")
        print(f"Evaluation device: {device}")
        print(f"Model type: {type(model_obj).__name__}")

        # Calculate VAE losses
        print("Calculating VAE losses...")
        beta = config.get('beta', 1.0)
        vae_losses = calculate_vae_losses(model_obj, test_loader_obj, device, beta)

        # Calculate reconstruction errors for anomaly detection
        print("Calculating reconstruction errors...")
        reconstruction_errors = calculate_reconstruction_error(model_obj, test_loader_obj, device)

        # Detect anomalies using different thresholds
        print("Performing anomaly detection...")
        anomaly_thresholds = [90, 95, 99]
        anomaly_results = {}
        
        for threshold in anomaly_thresholds:
            anomaly_results[f'threshold_{threshold}'] = detect_anomalies(reconstruction_errors, threshold)

        # Calculate summary statistics
        recon_stats = {
            'mean': float(np.mean(reconstruction_errors)),
            'std': float(np.std(reconstruction_errors)),
            'min': float(np.min(reconstruction_errors)),
            'max': float(np.max(reconstruction_errors)),
            'median': float(np.median(reconstruction_errors)),
            'percentile_90': float(np.percentile(reconstruction_errors, 90)),
            'percentile_95': float(np.percentile(reconstruction_errors, 95)),
            'percentile_99': float(np.percentile(reconstruction_errors, 99))
        }

        # Compile all metrics
        metrics = {
            'model_type': config.get('model_type', 'VAE'),
            'evaluation_config': {
                'beta': beta,
                'latent_dim': config.get('latent_dim', 16),
                'test_samples': len(reconstruction_errors)
            },
            'vae_losses': vae_losses,
            'reconstruction_error_stats': recon_stats,
            'anomaly_detection': anomaly_results,
            'model_info': {
                'total_parameters': sum(p.numel() for p in model_obj.parameters()),
                'trainable_parameters': sum(p.numel() for p in model_obj.parameters() if p.requires_grad)
            }
        }

        print("--- VAE Evaluation Results ---")
        print(f"Average ELBO Loss: {vae_losses['avg_elbo_loss']:.6f}")
        print(f"Average Reconstruction Loss: {vae_losses['avg_reconstruction_loss']:.6f}")
        print(f"Average KL Divergence: {vae_losses['avg_kl_divergence']:.6f}")
        print(f"Reconstruction Error - Mean: {recon_stats['mean']:.6f}, Std: {recon_stats['std']:.6f}")
        
        for threshold in anomaly_thresholds:
            result = anomaly_results[f'threshold_{threshold}']
            print(f"Anomalies at {threshold}th percentile: {result['num_anomalies']} ({result['anomaly_rate']:.2%})")

        print("--- Finished VAE Evaluation ---")

        # Save metrics
        os.makedirs(os.path.dirname(args.eval_metrics), exist_ok=True)
        with open(args.eval_metrics, "w") as f:
            json.dump(metrics, f, indent=2)

        print(f"Saved evaluation metrics to {args.eval_metrics}")

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_loader
      - {inputPath: test_loader}
      - --config
      - {inputValue: config}
      - --eval_metrics
      - {outputPath: eval_metrics}
