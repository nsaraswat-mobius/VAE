name: Preprocess VAE Dataset
description: Takes raw image data from DataLoaders, applies VAE-specific preprocessing, and outputs processed DataLoaders with metadata for all VAE types (Standard, Beta, Conditional, VQ-VAE).
inputs:
  - {name: train_data, type: Dataset, description: "Training data loader pickle file from data loader component"}
  - {name: val_data, type: Dataset, description: "Validation data loader pickle file from data loader component"}
  - {name: data_shape, type: Dataset, description: "Data shape from data loader component (height,width,channels)"}
  - {name: vae_type, type: Dataset, description: "Type of VAE: standard, beta, conditional, vqvae"}
  - {name: preprocessing_config, type: Dataset, description: "JSON string with preprocessing configuration"}
outputs:
  - {name: processed_train_data, type: Dataset, description: "Processed training DataLoader"}
  - {name: processed_val_data, type: Dataset, description: "Processed validation DataLoader"}
  - {name: processed_data_shape, type: Dataset, description: "Flattened input dimension for VAE"}
  - {name: preprocessing_metadata, type: Dataset, description: "Metadata about preprocessing operations"}
  - {name: class_mapping, type: Dataset, description: "Class labels mapping JSON string for conditional VAE"}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import torch
        import json
        import sys
        from torch.utils.data import DataLoader, Dataset

        class ProcessedVAEDataset(Dataset):
            def __init__(self, original_loader, vae_type):
                data_list = []
                label_list = []
                for batch_data, batch_labels in original_loader:
                    flat_data = batch_data.view(batch_data.size(0), -1)
                    data_list.append(flat_data)
                    label_list.append(batch_labels)
                
                self.data = torch.cat(data_list, dim=0)
                self.labels = torch.cat(label_list, dim=0)
                self.vae_type = vae_type
                self.class_mapping = {}
                
                self.apply_preprocessing()
            
            def apply_preprocessing(self):
                if self.vae_type == 'standard' or self.vae_type == 'beta':
                    if self.data.min() < 0:
                        self.data = (self.data + 1) / 2
                elif self.vae_type == 'conditional':
                    if self.data.min() < 0:
                        self.data = (self.data + 1) / 2
                    unique_labels = torch.unique(self.labels)
                    self.class_mapping = {int(label): idx for idx, label in enumerate(unique_labels)}
                    remapped = torch.zeros_like(self.labels)
                    for orig, new in self.class_mapping.items():
                        remapped[self.labels == orig] = new
                    self.labels = remapped
                elif self.vae_type == 'vqvae':
                    if self.data.max() <= 1 and self.data.min() >= 0:
                        self.data = self.data * 2 - 1
            
            def __len__(self):
                return len(self.data)
            
            def __getitem__(self, idx):
                return self.data[idx], self.labels[idx]

        print("Starting VAE preprocessing...")
        print(f"Arguments received: {sys.argv[1:]}")

        parser = argparse.ArgumentParser()
        parser.add_argument('--train_data', required=True)
        parser.add_argument('--val_data', required=True)
        parser.add_argument('--data_shape', required=True)
        parser.add_argument('--vae_type', required=True)
        parser.add_argument('--preprocessing_config', required=True)
        parser.add_argument('--processed_train_data', required=True)
        parser.add_argument('--processed_val_data', required=True)
        parser.add_argument('--processed_data_shape', required=True)
        parser.add_argument('--preprocessing_metadata', required=True)
        parser.add_argument('--class_mapping', required=True)
        args = parser.parse_args()

        print("=" * 60)
        print("VAE PREPROCESSING COMPONENT")
        print("=" * 60)
        
        # Read all input files
        print("\nReading input files...")
        
        try:
            with open(args.data_shape, 'r') as f:
                data_shape = f.read().strip().strip('"').strip("'")
            print(f"  Data shape: {data_shape}")
        except Exception as e:
            print(f"  Error reading data_shape: {e}")
            data_shape = "512,1,1"  # Default for failure signature
            print(f"  Using default: {data_shape}")
        
        try:
            with open(args.vae_type, 'r') as f:
                vae_type = f.read().strip().strip('"').strip("'")
            print(f"  VAE type: {vae_type}")
        except Exception as e:
            print(f"  Error reading vae_type: {e}")
            vae_type = "standard"  # Default
            print(f"  Using default: {vae_type}")
            
        try:
            with open(args.preprocessing_config, 'r') as f:
                preprocessing_config = f.read().strip()
            print(f"  Preprocessing config loaded")
        except Exception as e:
            print(f"  Error reading preprocessing_config: {e}")
            preprocessing_config = "{}"  # Default
            print(f"  Using default: {preprocessing_config}")

        print("\nLoading data loaders...")
        try:
            with open(args.train_data, 'rb') as f:
                train_loader = pickle.load(f)
            print(f"  Training data loaded successfully")
            
            with open(args.val_data, 'rb') as f:
                val_loader = pickle.load(f)
            print(f"  Validation data loaded successfully")
            
        except Exception as e:
            print(f"  Error loading data: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
            
        print("\nProcessing data...")
        processed_train = ProcessedVAEDataset(train_loader, vae_type)
        processed_val = ProcessedVAEDataset(val_loader, vae_type)
        print(f"  Training samples: {len(processed_train)}")
        print(f"  Validation samples: {len(processed_val)}")
        
        print("\nCreating new data loaders...")
        batch_size = train_loader.batch_size
        new_train_loader = DataLoader(processed_train, batch_size=batch_size, shuffle=True, num_workers=0)
        new_val_loader = DataLoader(processed_val, batch_size=batch_size, shuffle=False, num_workers=0)
        print(f"  Batch size: {batch_size}")
        
        print("\nCalculating dimensions and saving outputs...")
        h, w, c = map(int, data_shape.split(','))
        flattened_dim = h * w * c
        print(f"  Input shape: {h}x{w}x{c}")
        print(f"  Flattened dimension: {flattened_dim}")
        
        metadata = {
            'vae_type': vae_type,
            'flattened_input_dim': flattened_dim,
            'num_train_samples': len(processed_train),
            'num_val_samples': len(processed_val),
            'original_shape': data_shape
        }
        
        # Create output directories
        os.makedirs(os.path.dirname(args.processed_train_data) or '.', exist_ok=True)
        os.makedirs(os.path.dirname(args.processed_val_data) or '.', exist_ok=True)
        os.makedirs(os.path.dirname(args.processed_data_shape) or '.', exist_ok=True)
        os.makedirs(os.path.dirname(args.preprocessing_metadata) or '.', exist_ok=True)
        os.makedirs(os.path.dirname(args.class_mapping) or '.', exist_ok=True)
        
        # Save outputs
        with open(args.processed_train_data, 'wb') as f:
            pickle.dump(new_train_loader, f)
        print(f"  Saved: {args.processed_train_data}")
        
        with open(args.processed_val_data, 'wb') as f:
            pickle.dump(new_val_loader, f)
        print(f"  Saved: {args.processed_val_data}")
        
        with open(args.processed_data_shape, 'w') as f:
            f.write(str(flattened_dim))
        print(f"  Saved: {args.processed_data_shape} (value: {flattened_dim})")
        
        with open(args.preprocessing_metadata, 'wb') as f:
            pickle.dump(metadata, f)
        print(f"  Saved: {args.preprocessing_metadata}")
        
        with open(args.class_mapping, 'w') as f:
            f.write(json.dumps(processed_train.class_mapping))
        print(f"  Saved: {args.class_mapping}")
        
        print("\n" + "=" * 60)
        print("VAE PREPROCESSING COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        print(f"Summary:")
        print(f"  VAE Type: {vae_type}")
        print(f"  Input Dimension: {flattened_dim}")
        print(f"  Training Samples: {len(processed_train)}")
        print(f"  Validation Samples: {len(processed_val)}")
        print(f"  Batch Size: {batch_size}")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nCRITICAL ERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    args:
      - --train_data
      - {inputPath: train_data}
      - --val_data
      - {inputPath: val_data}
      - --data_shape
      - {inputPath: data_shape}
      - --vae_type
      - {inputPath: vae_type}
      - --preprocessing_config
      - {inputPath: preprocessing_config}
      - --processed_train_data
      - {outputPath: processed_train_data}
      - --processed_val_data
      - {outputPath: processed_val_data}
      - --processed_data_shape
      - {outputPath: processed_data_shape}
      - --preprocessing_metadata
      - {outputPath: preprocessing_metadata}
      - --class_mapping
      - {outputPath: class_mapping}
