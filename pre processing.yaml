name: Preprocess Failure Signature Dataset
description: Takes raw failure signature data from DataLoaders, applies VAE-specific preprocessing, and outputs processed DataLoaders with metadata for all VAE types.
inputs:
  - {name: train_data, type: Dataset, description: "Training data loader pickle file from data loader component"}
  - {name: val_data, type: Dataset, description: "Validation data loader pickle file from data loader component"}
  - {name: data_shape, type: Dataset, description: "Data shape from data loader component (height,width,channels)"}
  - {name: vae_type, type: Dataset, default: "standard", description: "Type of VAE: standard, beta, conditional, vqvae"}
  - {name: preprocessing_config, type: Dataset, default: "{}", description: "JSON string with preprocessing configuration"}
outputs:
  - {name: processed_train_data, type: Dataset, description: "Processed training DataLoader"}
  - {name: processed_val_data, type: Dataset, description: "Processed validation DataLoader"}
  - {name: processed_data_shape, type: String, description: "Flattened input dimension for VAE"}
  - {name: preprocessing_metadata, type: Dataset, description: "Metadata about preprocessing operations"}
  - {name: class_mapping, type: String, description: "Class labels mapping JSON string for conditional VAE"}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        set -e
        pip install torch torchvision pandas scikit-learn numpy --quiet
        
        cat > preprocess_vae_data.py << 'EOF'
        cat > preprocess_vae_data.py << 'EOF'
        import argparse
        import os
        import pickle
        import torch
        import json
        import numpy as np
        from torch.utils.data import DataLoader

        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)

        def preprocess_failure_data(train_path, val_path, data_shape_path):
            print("Loading failure signature data...")
            
            # Load data loaders
            with open(train_path, 'rb') as f:
                train_loader = pickle.load(f)
            with open(val_path, 'rb') as f:
                val_loader = pickle.load(f)
            
            # Read data shape
            with open(data_shape_path, 'r') as f:
                data_shape_str = f.read().strip().strip('"')
            
            print(f"Data shape: {data_shape_str}")
            
            # Extract all data from loaders
            train_data_list = []
            train_labels_list = []
            for batch_data, batch_labels in train_loader:
                flat_data = batch_data.view(batch_data.size(0), -1)
                train_data_list.append(flat_data)
                train_labels_list.append(batch_labels)
            
            val_data_list = []
            val_labels_list = []
            for batch_data, batch_labels in val_loader:
                flat_data = batch_data.view(batch_data.size(0), -1)
                val_data_list.append(flat_data)
                val_labels_list.append(batch_labels)
            
            # Concatenate all batches
            train_data = torch.cat(train_data_list, dim=0)
            train_labels = torch.cat(train_labels_list, dim=0)
            val_data = torch.cat(val_data_list, dim=0)
            val_labels = torch.cat(val_labels_list, dim=0)
            
            print(f"Train data shape: {train_data.shape}")
            print(f"Val data shape: {val_data.shape}")
            
            # Normalize data to [0, 1] range
            data_min = train_data.min()
            data_max = train_data.max()
            if data_min < 0 or data_max > 1:
                train_data = (train_data - data_min) / (data_max - data_min + 1e-8)
                val_data = (val_data - data_min) / (data_max - data_min + 1e-8)
                print(f"Data normalized from [{data_min:.4f}, {data_max:.4f}] to [0, 1]")
            
            # Calculate input dimension
            h, w, c = map(int, data_shape_str.split(','))
            input_dim = h * w * c
            
            # Create data wrapper object
            data_dict = {
                'train_data': train_data.numpy(),
                'train_labels': train_labels.numpy(),
                'val_data': val_data.numpy(),
                'val_labels': val_labels.numpy(),
                'input_dim': input_dim,
                'num_train': len(train_data),
                'num_val': len(val_data),
                'data_shape': data_shape_str,
                'normalization': {
                    'min': float(data_min),
                    'max': float(data_max)
                }
            }
            
            data_wrapper = DataWrapper(data_dict)
            
            print(f"Preprocessing complete:")
            print(f"  Train samples: {data_wrapper.num_train}")
            print(f"  Val samples: {data_wrapper.num_val}")
            print(f"  Input dimension: {data_wrapper.input_dim}")
            
            return data_wrapper

        if __name__ == "__main__":
            parser = argparse.ArgumentParser()
            parser.add_argument('--train_data', required=True)
            parser.add_argument('--val_data', required=True)
            parser.add_argument('--data_shape', required=True)
            parser.add_argument('--output_pickle', required=True)
            parser.add_argument('--weight_out', required=True)
            args = parser.parse_args()
            
            # Preprocess data
            data_wrapper = preprocess_failure_data(
                args.train_data,
                args.val_data,
                args.data_shape
            )
            
            # Save pickled data wrapper
            os.makedirs(os.path.dirname(args.output_pickle) or '.', exist_ok=True)
            with open(args.output_pickle, 'wb') as f:
                pickle.dump(data_wrapper, f)
            print(f"Saved processed data to: {args.output_pickle}")
            
            # Save metadata
            metadata = {
                'input_dim': data_wrapper.input_dim,
                'num_train': data_wrapper.num_train,
                'num_val': data_wrapper.num_val,
                'data_shape': data_wrapper.data_shape
            }
            os.makedirs(os.path.dirname(args.weight_out) or '.', exist_ok=True)
            with open(args.weight_out, 'w') as f:
                json.dump(metadata, f)
            print(f"Saved metadata to: {args.weight_out}")
        EOF
        
        python preprocess_vae_data.py \
          --train_data "$0" \
          --val_data "$1" \
          --data_shape "$2" \
          --output_pickle "$3" \
          --weight_out "$4"
    args:
      - {inputPath: train_data}
      - {inputPath: val_data}
      - {inputPath: data_shape}
      - {outputPath: processed_data_pickle}
      - {outputPath: weight_out}
