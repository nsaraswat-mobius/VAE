name: Preprocess Failure Signature Dataset
description: Takes raw failure signature data from DataLoaders, applies VAE-specific preprocessing, and outputs processed DataLoaders with metadata for all VAE types.
inputs:
  - {name: train_data, type: Dataset, description: "Training data loader pickle file from data loader component"}
  - {name: val_data, type: Dataset, description: "Validation data loader pickle file from data loader component"}
  - {name: data_shape, type: Dataset, description: "Data shape from data loader component (height,width,channels)"}
  - {name: vae_type, type: Dataset, default: "standard", description: "Type of VAE: standard, beta, conditional, vqvae"}
  - {name: preprocessing_config, type: Dataset, default: "{}", description: "JSON string with preprocessing configuration"}
outputs:
  - {name: processed_train_data, type: Dataset, description: "Processed training DataLoader"}
  - {name: processed_val_data, type: Dataset, description: "Processed validation DataLoader"}
  - {name: processed_data_shape, type: String, description: "Flattened input dimension for VAE"}
  - {name: preprocessing_metadata, type: Dataset, description: "Metadata about preprocessing operations"}
  - {name: class_mapping, type: String, description: "Class labels mapping JSON string for conditional VAE"}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        set -e
        pip install torch torchvision pandas scikit-learn numpy --quiet
        
        cat > preprocess_vae_data.py << 'EOF'
        import argparse
        import os
        import pickle
        import torch
        import json
        import numpy as np
        from torch.utils.data import DataLoader, TensorDataset

        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)

        def preprocess_failure_data(train_path, val_path, data_shape_path, vae_type_path, config_path):
            print("Loading failure signature data...")
            
            # Load data loaders
            with open(train_path, 'rb') as f:
                train_loader = pickle.load(f)
            with open(val_path, 'rb') as f:
                val_loader = pickle.load(f)
            
            # Read data shape
            with open(data_shape_path, 'r') as f:
                data_shape_str = f.read().strip().strip('"').strip("'")
            
            # Read vae type
            try:
                with open(vae_type_path, 'r') as f:
                    vae_type = f.read().strip().strip('"').strip("'")
            except:
                vae_type = "standard"
            
            # Read config
            try:
                with open(config_path, 'r') as f:
                    config = f.read().strip()
            except:
                config = "{}"
            
            print(f"Data shape: {data_shape_str}")
            print(f"VAE type: {vae_type}")
            
            # Extract all data from loaders
            train_data_list = []
            train_labels_list = []
            for batch_data, batch_labels in train_loader:
                flat_data = batch_data.view(batch_data.size(0), -1)
                train_data_list.append(flat_data)
                train_labels_list.append(batch_labels)
            
            val_data_list = []
            val_labels_list = []
            for batch_data, batch_labels in val_loader:
                flat_data = batch_data.view(batch_data.size(0), -1)
                val_data_list.append(flat_data)
                val_labels_list.append(batch_labels)
            
            # Concatenate all batches
            train_data = torch.cat(train_data_list, dim=0)
            train_labels = torch.cat(train_labels_list, dim=0)
            val_data = torch.cat(val_data_list, dim=0)
            val_labels = torch.cat(val_labels_list, dim=0)
            
            print(f"Train data shape: {train_data.shape}")
            print(f"Val data shape: {val_data.shape}")
            
            # Apply preprocessing based on VAE type
            class_mapping = {}
            
            if vae_type == 'standard' or vae_type == 'beta':
                # Normalize to [0, 1] if needed
                if train_data.min() < 0:
                    train_data = (train_data + 1) / 2
                    val_data = (val_data + 1) / 2
                    print("Data normalized to [0, 1]")
            
            elif vae_type == 'conditional':
                # Normalize to [0, 1] if needed
                if train_data.min() < 0:
                    train_data = (train_data + 1) / 2
                    val_data = (val_data + 1) / 2
                
                # Create class mapping
                unique_labels = torch.unique(train_labels)
                class_mapping = {int(label): idx for idx, label in enumerate(unique_labels)}
                remapped_train = torch.zeros_like(train_labels)
                remapped_val = torch.zeros_like(val_labels)
                for orig, new in class_mapping.items():
                    remapped_train[train_labels == orig] = new
                    remapped_val[val_labels == orig] = new
                train_labels = remapped_train
                val_labels = remapped_val
                print(f"Class mapping created: {class_mapping}")
            
            elif vae_type == 'vqvae':
                # Normalize to [-1, 1] if in [0, 1]
                if train_data.max() <= 1 and train_data.min() >= 0:
                    train_data = train_data * 2 - 1
                    val_data = val_data * 2 - 1
                    print("Data normalized to [-1, 1]")
            
            # Calculate input dimension
            h, w, c = map(int, data_shape_str.split(','))
            input_dim = h * w * c
            
            # Create new DataLoaders
            train_dataset = TensorDataset(train_data, train_labels)
            val_dataset = TensorDataset(val_data, val_labels)
            
            batch_size = train_loader.batch_size
            new_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
            new_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
            
            # Create metadata
            metadata = {
                'vae_type': vae_type,
                'input_dim': input_dim,
                'num_train': len(train_data),
                'num_val': len(val_data),
                'data_shape': data_shape_str,
                'batch_size': batch_size
            }
            
            print(f"Preprocessing complete:")
            print(f"  Train samples: {metadata['num_train']}")
            print(f"  Val samples: {metadata['num_val']}")
            print(f"  Input dimension: {metadata['input_dim']}")
            print(f"  Batch size: {batch_size}")
            
            return new_train_loader, new_val_loader, input_dim, metadata, class_mapping

        if __name__ == "__main__":
            parser = argparse.ArgumentParser()
            parser.add_argument('--train_data', required=True)
            parser.add_argument('--val_data', required=True)
            parser.add_argument('--data_shape', required=True)
            parser.add_argument('--vae_type', required=True)
            parser.add_argument('--preprocessing_config', required=True)
            parser.add_argument('--processed_train_data', required=True)
            parser.add_argument('--processed_val_data', required=True)
            parser.add_argument('--processed_data_shape', required=True)
            parser.add_argument('--preprocessing_metadata', required=True)
            parser.add_argument('--class_mapping', required=True)
            args = parser.parse_args()
            
            # Preprocess data
            train_loader, val_loader, input_dim, metadata, class_mapping = preprocess_failure_data(
                args.train_data,
                args.val_data,
                args.data_shape,
                args.vae_type,
                args.preprocessing_config
            )
            
            # Create output directories
            os.makedirs(os.path.dirname(args.processed_train_data) or '.', exist_ok=True)
            os.makedirs(os.path.dirname(args.processed_val_data) or '.', exist_ok=True)
            os.makedirs(os.path.dirname(args.processed_data_shape) or '.', exist_ok=True)
            os.makedirs(os.path.dirname(args.preprocessing_metadata) or '.', exist_ok=True)
            os.makedirs(os.path.dirname(args.class_mapping) or '.', exist_ok=True)
            
            # Save processed train data
            with open(args.processed_train_data, 'wb') as f:
                pickle.dump(train_loader, f)
            print(f"Saved processed train data to: {args.processed_train_data}")
            
            # Save processed val data
            with open(args.processed_val_data, 'wb') as f:
                pickle.dump(val_loader, f)
            print(f"Saved processed val data to: {args.processed_val_data}")
            
            # Save processed data shape
            with open(args.processed_data_shape, 'w') as f:
                f.write(str(input_dim))
            print(f"Saved input dimension: {input_dim}")
            
            # Save preprocessing metadata
            with open(args.preprocessing_metadata, 'wb') as f:
                pickle.dump(metadata, f)
            print(f"Saved metadata to: {args.preprocessing_metadata}")
            
            # Save class mapping
            with open(args.class_mapping, 'w') as f:
                json.dump(class_mapping, f)
            print(f"Saved class mapping to: {args.class_mapping}")
            
            print("VAE preprocessing completed successfully!")
        EOF
        
        python preprocess_vae_data.py
    args:
      - --train_data
      - {inputPath: train_data}
      - --val_data
      - {inputPath: val_data}
      - --data_shape
      - {inputPath: data_shape}
      - --vae_type
      - {inputPath: vae_type}
      - --preprocessing_config
      - {inputPath: preprocessing_config}
      - --processed_train_data
      - {outputPath: processed_train_data}
      - --processed_val_data
      - {outputPath: processed_val_data}
      - --processed_data_shape
      - {outputPath: processed_data_shape}
      - --preprocessing_metadata
      - {outputPath: preprocessing_metadata}
      - --class_mapping
      - {outputPath: class_mapping}
