name: Train VAE Model
description: Trains the VAE model with Traditional, CAFO, or Forward Forward methods.
inputs:
  - {name: model, type: Model}
  - {name: train_loader, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: trained_model, type: Model}
  - {name: epoch_loss, type: String}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v30
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        import torch
        import torch.nn as nn
        import torch.optim as optim
        import uuid
        
        from nesy_factory.VAE.standard_vae import StandardVAE

        parser = argparse.ArgumentParser()
        parser.add_argument('--model', type=str, required=True)
        parser.add_argument('--train_loader', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--epoch_loss', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)

        with open(args.model, 'rb') as f:
            model_obj = pickle.load(f)

        with open(args.train_loader, 'rb') as f:
            train_data = pickle.load(f)
            
        if isinstance(train_data, dict) and 'loader' in train_data:
            train_loader_obj = train_data['loader']
            metadata = train_data['metadata']
            print("Loaded training data with metadata")
        else:
            train_loader_obj = train_data
            metadata = None
            print("Loaded training data legacy format")

        print("Starting VAE Model Training")
        epoch_loss_data = []

        if not hasattr(model_obj, 'use_cafo'):
            model_obj.use_cafo = False
            print("Backward compatibility: Setting use_cafo=False for existing model")
        
        if not hasattr(model_obj, 'use_forward_forward'):
            model_obj.use_forward_forward = False
            print("Backward compatibility: Setting use_forward_forward=False for existing model")

        use_cafo_from_config = config.get('use_cafo', False)
        use_ff_from_config = config.get('use_forward_forward', False)
        
        if sum([use_cafo_from_config, use_ff_from_config]) > 1:
            raise ValueError("Only one training method can be selected: use_cafo or use_forward_forward")
        
        X_train_list = []
        for batch_data in train_loader_obj:
            if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 1:
                data = batch_data[0]
            else:
                data = batch_data
            X_train_list.append(data)
        
        X_train = torch.cat(X_train_list, dim=0)
        actual_input_dim = X_train.shape[1]
        print(f"Training data shape: X={X_train.shape}")
        print(f"Actual input dimension: {actual_input_dim}")
        
        if use_cafo_from_config and not model_obj.use_cafo:
            print("Warning: Config requests CAFO but model was created without CAFO support.")
            print("Creating new CAFO model with same architecture...")
            requested_cafo_blocks = config.get('cafo_blocks', len(model_obj.hidden_dims))
            start_dim = actual_input_dim
            end_dim = model_obj.latent_dim
            if requested_cafo_blocks == 1:
                import math
                hidden_dim = int(math.sqrt(start_dim * end_dim))
                if hidden_dim == start_dim or hidden_dim == end_dim:
                    hidden_dim = int((start_dim + end_dim) / 2)
                new_hidden_dims = [hidden_dim]
            elif requested_cafo_blocks == 2:
                import math
                ratio = (end_dim / start_dim) ** (1.0 / 3)
                dim1 = int(start_dim * ratio)
                dim2 = int(start_dim * ratio * ratio)
                dim1 = max(dim1, end_dim + 3)
                dim2 = max(dim2, end_dim + 1)
                if dim1 <= dim2:
                    dim1 = dim2 + 2
                new_hidden_dims = [dim1, dim2]
            else:
                new_hidden_dims = []
                step = (start_dim - end_dim) / (requested_cafo_blocks + 1)
                for i in range(requested_cafo_blocks):
                    dim = int(start_dim - step * (i + 1))
                    dim = max(dim, end_dim)
                    new_hidden_dims.append(dim)
            new_config = {
                'input_dim': actual_input_dim,
                'latent_dim': model_obj.latent_dim,
                'hidden_dims': new_hidden_dims,
                'learning_rate': config.get('learning_rate', 0.001),
                'beta': config.get('beta', 1.0),
                'use_cafo': True,
                'cafo_blocks': requested_cafo_blocks,
                'epochs_per_block': config.get('epochs_per_block', 50),
                'block_lr': config.get('block_lr', 0.001)
            }
            model_obj = StandardVAE(new_config)
        elif use_ff_from_config and not model_obj.use_forward_forward:
            print("Creating new Forward Forward VAE model...")
            
            # Get FF parameters
            ff_blocks = config.get('ff_blocks', 2)
            ff_threshold = config.get('ff_threshold', 1.5)
            ff_epochs_per_block = config.get('ff_epochs_per_block', 20)
            ff_lr = config.get('ff_lr', 0.01)
            
            # Create simpler architecture for FF
            if ff_blocks == 1:
                hidden_dims = [64]
            elif ff_blocks == 2:
                hidden_dims = [128, 64]
            elif ff_blocks == 3:
                hidden_dims = [256, 128, 64]
            else:
                hidden_dims = [256, 128, 64]
                ff_blocks = 3
            
            new_config = {
                'input_dim': actual_input_dim,
                'latent_dim': model_obj.latent_dim,
                'hidden_dims': hidden_dims,
                'learning_rate': config.get('learning_rate', 0.001),
                'beta': 0.5,
                'use_forward_forward': True,
                'use_cafo': False,
                'ff_blocks': ff_blocks,
                'ff_threshold': ff_threshold,
                'ff_epochs_per_block': ff_epochs_per_block,
                'ff_lr': ff_lr,
                'ff_use_batch_norm': True,
                'ff_dropout': 0.1
            }
            
            model_obj = StandardVAE(new_config)
            print(f"Created FF-VAE with {ff_blocks} blocks, architecture: {actual_input_dim} -> {hidden_dims} -> {model_obj.latent_dim}")
        
        if hasattr(model_obj, 'input_dim') and actual_input_dim != model_obj.input_dim:
            model_obj.input_dim = actual_input_dim

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model_obj = model_obj.to(device)
        X_train = X_train.to(device)
        print(f"Training VAE on device: {device}")

        # EMERGENCY PATCH: If FF training, fix initialization
        if hasattr(model_obj, 'use_forward_forward') and model_obj.use_forward_forward:
            print("Applying FF initialization patch...")
            
            # Reinitialize all layers with smaller weights
            for name, param in model_obj.named_parameters():
                if 'weight' in name:
                    nn.init.xavier_uniform_(param.data, gain=0.5)
                elif 'bias' in name:
                    nn.init.constant_(param.data, 0.01)
            
            # Also initialize batch norm layers if they exist
            for module in model_obj.modules():
                if isinstance(module, nn.BatchNorm1d):
                    module.weight.data.fill_(1.0)
                    module.bias.data.zero_()
            
            print("FF initialization complete")

        if hasattr(model_obj, 'use_forward_forward') and model_obj.use_forward_forward:
            print("Using Forward Forward training mode")
            
            # DEBUG: Check model structure
            print("Model structure check:")
            print(f"  Input dim: {model_obj.input_dim}")
            print(f"  Hidden dims: {model_obj.hidden_dims}")
            print(f"  Latent dim: {model_obj.latent_dim}")
            print(f"  Has train_forward_forward method: {hasattr(model_obj, 'train_forward_forward')}")
            
            # DEBUG: Normalize data for FF training
            print(f"Data stats before normalization - Mean: {X_train.mean().item():.4f}, Std: {X_train.std().item():.4f}")
            
            # Normalize data to prevent NaN
            X_mean = X_train.mean(dim=0, keepdim=True)
            X_std = X_train.std(dim=0, keepdim=True) + 1e-8
            X_train_normalized = (X_train - X_mean) / X_std
            print(f"Data stats after normalization - Mean: {X_train_normalized.mean().item():.4f}, Std: {X_train_normalized.std().item():.4f}")
            
            try:
                # Try with normalized data
                ff_results = model_obj.train_forward_forward(X_train_normalized, verbose=True)
                
                # Extract results
                if ff_results and 'encoder_results' in ff_results:
                    for i, block_result in enumerate(ff_results['encoder_results']):
                        if 'losses' in block_result:
                            for epoch, loss in enumerate(block_result['losses']):
                                if not torch.isnan(torch.tensor(loss)):
                                    epoch_loss_data.append({
                                        'block': i + 1,
                                        'block_type': 'encoder',
                                        'epoch': epoch + 1,
                                        'loss': float(loss),
                                        'training_mode': 'forward_forward'
                                    })
                
                if ff_results and 'decoder_results' in ff_results:
                    for i, block_result in enumerate(ff_results['decoder_results']):
                        if 'losses' in block_result:
                            for epoch, loss in enumerate(block_result['losses']):
                                if not torch.isnan(torch.tensor(loss)):
                                    epoch_loss_data.append({
                                        'block': i + 1,
                                        'block_type': 'decoder',
                                        'epoch': epoch + 1,
                                        'loss': float(loss),
                                        'training_mode': 'forward_forward'
                                    })
                
                print(f"Forward Forward training completed successfully")
                
            except Exception as e:
                print(f"Forward Forward training failed: {str(e)}")
                import traceback
                traceback.print_exc()
                
                print("Falling back to traditional VAE training...")
                use_ff_from_config = False
                fallback_config = {
                    'input_dim': actual_input_dim,
                    'latent_dim': model_obj.latent_dim,
                    'hidden_dims': [128, 64],
                    'learning_rate': config.get('learning_rate', 0.001),
                    'beta': 1.0,
                    'use_cafo': False,
                    'use_forward_forward': False
                }
                model_obj = StandardVAE(fallback_config).to(device)
        
        if hasattr(model_obj, 'use_cafo') and model_obj.use_cafo and use_cafo_from_config:
            try:
                cafo_results = model_obj.train_cafo(X_train, verbose=True)
                for i, block_result in enumerate(cafo_results['encoder_results']):
                    for epoch, loss in enumerate(block_result['train_losses']):
                        epoch_loss_data.append({
                            'block': i + 1,
                            'block_type': 'encoder',
                            'epoch': epoch + 1,
                            'loss': float(loss),
                            'training_mode': 'cafo'
                        })
                for i, block_result in enumerate(cafo_results['decoder_results']):
                    for epoch, loss in enumerate(block_result['train_losses']):
                        epoch_loss_data.append({
                            'block': i + 1,
                            'block_type': 'decoder',
                            'epoch': epoch + 1,
                            'loss': float(loss),
                            'training_mode': 'cafo'
                        })
            except Exception as e:
                print(f"CAFO training failed: {str(e)}")
                use_cafo_from_config = False
                fallback_config = {
                    'input_dim': actual_input_dim,
                    'latent_dim': model_obj.latent_dim,
                    'hidden_dims': [256, 128],
                    'learning_rate': config.get('learning_rate', 0.001),
                    'beta': config.get('beta', 1.0),
                    'use_cafo': False,
                    'use_forward_forward': False
                }
                model_obj = StandardVAE(fallback_config).to(device)
        
        if not use_cafo_from_config and not use_ff_from_config:
            learning_rate = config.get('learning_rate', 0.001)
            epochs = config.get('epochs', 100)
            beta = config.get('beta', 1.0)
            optimizer = optim.Adam(model_obj.parameters(), lr=learning_rate)
            model_obj.train()
            for epoch in range(epochs):
                total_loss, total_recon_loss, total_kl_loss, num_batches = 0, 0, 0, 0
                for batch_data in train_loader_obj:
                    data = batch_data[0] if isinstance(batch_data, (list, tuple)) else batch_data
                    data = data.to(device)
                    optimizer.zero_grad()
                    vae_output = model_obj(data)
                    if isinstance(vae_output, tuple) and len(vae_output) == 3:
                        recon_data, mu, logvar = vae_output
                    elif isinstance(vae_output, dict):
                        recon_data = vae_output.get('reconstruction', data)
                        mu = vae_output.get('mu', torch.zeros(data.size(0), model_obj.latent_dim).to(device))
                        logvar = vae_output.get('logvar', torch.zeros(data.size(0), model_obj.latent_dim).to(device))
                    else:
                        recon_data, mu, logvar = vae_output, torch.zeros(data.size(0), model_obj.latent_dim).to(device), torch.zeros(data.size(0), model_obj.latent_dim).to(device)
                    recon_loss = nn.MSELoss()(recon_data, data)
                    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / data.size(0)
                    loss = recon_loss + beta * kl_loss
                    loss.backward()
                    optimizer.step()
                    total_loss += loss.item()
                    total_recon_loss += recon_loss.item()
                    total_kl_loss += kl_loss.item()
                    num_batches += 1
                if num_batches > 0:
                    avg_loss = total_loss / num_batches
                    epoch_loss_data.append({
                        'epoch': epoch + 1,
                        'loss': avg_loss,
                        'recon_loss': total_recon_loss / num_batches,
                        'kl_loss': total_kl_loss / num_batches,
                        'training_mode': 'traditional'
                    })

        training_mode = 'forward_forward' if (hasattr(model_obj, 'use_forward_forward') and model_obj.use_forward_forward and use_ff_from_config) else \
                       'cafo' if (hasattr(model_obj, 'use_cafo') and model_obj.use_cafo and use_cafo_from_config) else 'traditional'

        output_dir_trained_model = os.path.dirname(args.trained_model)
        if output_dir_trained_model and not os.path.exists(output_dir_trained_model):
            os.makedirs(output_dir_trained_model, exist_ok=True)
        with open(args.trained_model, 'wb') as f:
            pickle.dump(model_obj.cpu(), f)

        for entry in epoch_loss_data:
            entry['uid'] = str(uuid.uuid4())

        output_dir_epoch_loss = os.path.dirname(args.epoch_loss)
        if output_dir_epoch_loss and not os.path.exists(output_dir_epoch_loss):
            os.makedirs(output_dir_epoch_loss, exist_ok=True)
        with open(args.epoch_loss, 'w') as f:
            json.dump(epoch_loss_data, f, indent=2)

        print(f"Saved trained VAE model to {args.trained_model}")
        print(f"Saved training summary to {args.epoch_loss}")
        print(f"Training method used: {training_mode.upper()}")
    args:
      - --model
      - {inputPath: model}
      - --train_loader
      - {inputPath: train_loader}
      - --config
      - {inputValue: config}
      - --trained_model
      - {outputPath: trained_model}
      - --epoch_loss
      - {outputPath: epoch_loss}
