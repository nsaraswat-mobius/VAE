name: VAE Evolve Model
description: VAE Evolves the trained model using evolutionary algorithms and optimization techniques.
inputs:
  - {name: trained_model, type: Model}         # Trained VAE model from training brick
  - {name: data_path, type: Dataset}           # Training data for evolution evaluation
  - {name: config, type: String}               # Evolution configuration (generations, etc.)
outputs:
  - {name: evolved_model, type: Model}         # Evolved model object
  - {name: evolution_history, type: String}    # Evolution metrics and history
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse
        import pickle
        import os
        import json
        import numpy as np
        from nesy_factory.VAE import create_vae_model
        from nesy_factory.utils import set_random_seed
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import DataLoader, Dataset
        import random
        import copy
        
        class ProcessedVAEDataset(Dataset):
            def __init__(self, original_loader, vae_type):
                data_list = []
                label_list = []
                for batch_data, batch_labels in original_loader:
                    flat_data = batch_data.view(batch_data.size(0), -1)
                    data_list.append(flat_data)
                    label_list.append(batch_labels)
                
                self.data = torch.cat(data_list, dim=0)
                self.labels = torch.cat(label_list, dim=0)
                self.vae_type = vae_type
                self.class_mapping = {}
                
                self.apply_preprocessing()
            
            def apply_preprocessing(self):
                if self.vae_type == 'standard' or self.vae_type == 'beta':
                    if self.data.min() < 0:
                        self.data = (self.data + 1) / 2
                elif self.vae_type == 'conditional':
                    if self.data.min() < 0:
                        self.data = (self.data + 1) / 2
                    unique_labels = torch.unique(self.labels)
                    self.class_mapping = {int(label): idx for idx, label in enumerate(unique_labels)}
                    remapped = torch.zeros_like(self.labels)
                    for orig, new in self.class_mapping.items():
                        remapped[self.labels == orig] = new
                    self.labels = remapped
                elif self.vae_type == 'vqvae':
                    if self.data.max() <= 1 and self.data.min() >= 0:
                        self.data = self.data * 2 - 1
            
            def __len__(self):
                return len(self.data)
            
            def __getitem__(self, idx):
                return self.data[idx], self.labels[idx]
        
        class DataWrapper:
          def __init__(self, data_dict):
              self.__dict__.update(data_dict)

        def evaluate_model(model, data_loader, device, vae_type='standard_vae', beta=1.0):
            model.eval()
            total_loss = 0
            num_samples = 0
            
            with torch.no_grad():
                for data_batch, _ in data_loader:
                    data_batch = data_batch.to(device)
                    
                    if vae_type == 'conditional_vae':
                        labels = _.to(device)
                        recon_batch, mu, logvar = model(data_batch, labels)
                    elif vae_type == 'vq_vae':
                        recon_batch, vq_loss, _ = model(data_batch)
                        loss = nn.functional.mse_loss(recon_batch, data_batch) + vq_loss
                        total_loss += loss.item() * data_batch.size(0)
                        num_samples += data_batch.size(0)
                        continue
                    else:
                        recon_batch, mu, logvar = model(data_batch)
                    
                    if vae_type != 'vq_vae':
                        recon_loss = nn.functional.mse_loss(recon_batch, data_batch, reduction='sum')
                        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                        if vae_type == 'beta_vae':
                            loss = recon_loss + beta * kl_loss
                        else:
                            loss = recon_loss + kl_loss
                        
                        total_loss += loss.item()
                        num_samples += data_batch.size(0)
            
            return total_loss / num_samples

        def mutate_model(model, mutation_rate=0.01, mutation_strength=0.1):
            mutated_model = copy.deepcopy(model)
            
            for param in mutated_model.parameters():
                if random.random() < mutation_rate:
                    noise = torch.randn_like(param) * mutation_strength
                    param.data += noise
            
            return mutated_model

        def crossover_models(parent1, parent2, crossover_rate=0.5):
            child = copy.deepcopy(parent1)
            
            for (name1, param1), (name2, param2) in zip(parent1.named_parameters(), parent2.named_parameters()):
                if name1 == name2:
                    mask = torch.rand_like(param1) < crossover_rate
                    child_param = getattr(child, name1.split('.')[0])
                    for attr in name1.split('.')[1:]:
                        child_param = getattr(child_param, attr)
                    child_param.data = torch.where(mask, param1.data, param2.data)
            
            return child

        # Parse arguments
        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--data_path', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--evolved_model', type=str, required=True)
        parser.add_argument('--evolution_history', type=str, required=True)
        args = parser.parse_args()

        print(f"Trained model path: {args.trained_model}")
        print(f"Data path: {args.data_path}")
        print(f"Config: {args.config}")
        print(f"Output evolved model: {args.evolved_model}")
        
        # Load data
        try:
            with open(args.data_path, "rb") as f:
                data_loader = pickle.load(f)
            print(f"Successfully loaded data. Type: {type(data_loader)}")
                
        except Exception as e:
            print(f"Error loading data: {e}")
            import traceback
            traceback.print_exc()
            exit(1)
            
        # Load and parse config
        print("Loading config...")
        try: 
            config = json.loads(args.config)
        except : 
            with open(args.config) as f:
                config = json.load(f)
        print(f"Evolution configs: {config}")
        
        # Extract evolution parameters
        vae_type = config.get('vae_type', 'standard_vae')
        generations = config.get('generations', 10)
        population_size = config.get('population_size', 5)
        mutation_rate = config.get('mutation_rate', 0.01)
        mutation_strength = config.get('mutation_strength', 0.1)
        crossover_rate = config.get('crossover_rate', 0.5)
        beta = config.get('beta', 1.0)
        
        print(f"VAE Type: {vae_type}")
        print(f"Generations: {generations}")
        print(f"Population Size: {population_size}")
        print(f"Mutation Rate: {mutation_rate}")

        # Load trained model
        print("Loading trained model...")
        try:
            # Create model architecture
            input_dim = config.get('input_dim', 784)
            hidden_dim = config.get('hidden_dim', 512)
            latent_dim = config.get('latent_dim', 128)
            
            if vae_type == 'standard_vae':
                class StandardVAE(nn.Module):
                    def __init__(self, input_dim, hidden_dim, latent_dim):
                        super(StandardVAE, self).__init__()
                        self.input_dim = input_dim
                        self.hidden_dim = hidden_dim
                        self.latent_dim = latent_dim
                        self.model_type = "standard_vae"
                        
                        # Encoder
                        self.encoder = nn.Sequential(
                            nn.Linear(input_dim, hidden_dim),
                            nn.ReLU(),
                            nn.Linear(hidden_dim, hidden_dim // 2),
                            nn.ReLU()
                        )
                        self.mu_layer = nn.Linear(hidden_dim // 2, latent_dim)
                        self.logvar_layer = nn.Linear(hidden_dim // 2, latent_dim)
                        
                        # Decoder
                        self.decoder = nn.Sequential(
                            nn.Linear(latent_dim, hidden_dim // 2),
                            nn.ReLU(),
                            nn.Linear(hidden_dim // 2, hidden_dim),
                            nn.ReLU(),
                            nn.Linear(hidden_dim, input_dim),
                            nn.Sigmoid()
                        )
                    
                    def encode(self, x):
                        h = self.encoder(x)
                        mu = self.mu_layer(h)
                        logvar = self.logvar_layer(h)
                        return mu, logvar
                    
                    def reparameterize(self, mu, logvar):
                        std = torch.exp(0.5 * logvar)
                        eps = torch.randn_like(std)
                        return mu + eps * std
                    
                    def decode(self, z):
                        return self.decoder(z)
                    
                    def forward(self, x):
                        mu, logvar = self.encode(x)
                        z = self.reparameterize(mu, logvar)
                        return self.decode(z), mu, logvar
                
                base_model = StandardVAE(input_dim, hidden_dim, latent_dim)
            else:
                base_model = create_vae_model(vae_type, config)
            
            # Load trained weights
            base_model.load_state_dict(torch.load(args.trained_model, map_location=torch.device('cpu')))
            print(f"Successfully loaded {vae_type} model")
            
        except Exception as e:
            print(f"Error loading model: {e}")
            exit(1)

        # Set device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        base_model = base_model.to(device)
        print(f"Using device: {device}")

        print(f"Starting VAE evolution for {generations} generations...")
        
        # Initialize population
        population = [copy.deepcopy(base_model) for _ in range(population_size)]
        
        evolution_history = []
        best_fitness = float('inf')
        best_model = None
        
        for generation in range(generations):
            print(f"Generation {generation + 1}/{generations}")
            
            # Evaluate population
            fitness_scores = []
            for i, individual in enumerate(population):
                fitness = evaluate_model(individual, data_loader, device, vae_type, beta)
                fitness_scores.append(fitness)
                print(f"  Individual {i+1}: Fitness = {fitness:.6f}")
            
            # Track best individual
            gen_best_idx = np.argmin(fitness_scores)
            gen_best_fitness = fitness_scores[gen_best_idx]
            
            if gen_best_fitness < best_fitness:
                best_fitness = gen_best_fitness
                best_model = copy.deepcopy(population[gen_best_idx])
                print(f"  New best fitness: {best_fitness:.6f}")
            
            # Record generation statistics
            generation_stats = {
                'generation': generation + 1,
                'best_fitness': gen_best_fitness,
                'avg_fitness': np.mean(fitness_scores),
                'worst_fitness': np.max(fitness_scores),
                'fitness_std': np.std(fitness_scores),
                'global_best_fitness': best_fitness
            }
            evolution_history.append(generation_stats)
            
            # Selection and reproduction
            if generation < generations - 1:  # Don't evolve on last generation
                # Tournament selection
                new_population = []
                
                # Keep best individual (elitism)
                new_population.append(copy.deepcopy(population[gen_best_idx]))
                
                # Generate rest of population
                while len(new_population) < population_size:
                    # Tournament selection
                    tournament_size = min(3, population_size)
                    tournament_indices = random.sample(range(population_size), tournament_size)
                    tournament_fitness = [fitness_scores[i] for i in tournament_indices]
                    winner_idx = tournament_indices[np.argmin(tournament_fitness)]
                    
                    # Create offspring
                    if random.random() < 0.7:  # Crossover probability
                        # Select second parent
                        parent2_indices = random.sample(range(population_size), tournament_size)
                        parent2_fitness = [fitness_scores[i] for i in parent2_indices]
                        parent2_idx = parent2_indices[np.argmin(parent2_fitness)]
                        
                        # Crossover
                        offspring = crossover_models(population[winner_idx], population[parent2_idx], crossover_rate)
                    else:
                        offspring = copy.deepcopy(population[winner_idx])
                    
                    # Mutation
                    offspring = mutate_model(offspring, mutation_rate, mutation_strength)
                    new_population.append(offspring)
                
                population = new_population

        print("Finished VAE evolution!")
        print(f"Best fitness achieved: {best_fitness:.6f}")
        
        # Save evolution history
        output_dir_history = os.path.dirname(args.evolution_history)
        if output_dir_history and not os.path.exists(output_dir_history):
            os.makedirs(output_dir_history, exist_ok=True)
        
        with open(args.evolution_history, 'w') as f:
            json.dump(evolution_history, f, indent=2)
        print(f"Saved evolution history to {args.evolution_history}")
        
        # Save best evolved model
        print("Saving evolved model...")
        try:
            output_dir = os.path.dirname(args.evolved_model)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
                print(f"Created directory: {output_dir}")
            
            torch.save(best_model.state_dict(), args.evolved_model)
            print(f"Saved evolved VAE model to {args.evolved_model}")
            
        except Exception as e:
            print(f"Error saving evolved model: {e}")
            exit(1)

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --evolved_model
      - {outputPath: evolved_model}
      - --evolution_history
      - {outputPath: evolution_history}
