name: build_ladder_vae_model
description: Initializes Ladder VAE model with hierarchical latent structure and saves architecture
inputs:
  - {name: num_features, type: Integer, description: "Number of input features", default: "16197"}
  - {name: level1_latent, type: Integer, description: "First latent level dimension", default: "32"}
  - {name: level2_latent, type: Integer, description: "Second latent level (bottleneck) dimension", default: "16"}
  - {name: level1_hidden, type: Integer, description: "First hidden layer dimension", default: "128"}
  - {name: level2_hidden, type: Integer, description: "Second hidden layer dimension", default: "64"}
  - {name: beta, type: Float, description: "KL divergence weight", default: "1.0"}
outputs:
  - {name: model_path, type: String, description: "Path to saved model checkpoint"}
  - {name: config_path, type: String, description: "Path to model config JSON"}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v2
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        
        # ===== EMBEDDED LADDER VAE MODULE =====
        class LadderVAE(nn.Module):
            
            def __init__(self, input_dim, level1_latent=32, level2_latent=16, 
                         level1_hidden=128, level2_hidden=64, beta=1.0):
                super(LadderVAE, self).__init__()
                self.level1_latent = level1_latent
                self.level2_latent = level2_latent
                self.level1_hidden = level1_hidden
                self.level2_hidden = level2_hidden
                self.beta = beta
                
                # Encoder Level 1
                self.enc1_fc = nn.Linear(input_dim, level1_hidden)
                self.enc1_mu = nn.Linear(level1_hidden, level1_latent)
                self.enc1_logvar = nn.Linear(level1_hidden, level1_latent)
                
                # Encoder Level 2
                self.enc2_fc = nn.Linear(level1_latent, level2_hidden)
                self.enc2_mu = nn.Linear(level2_hidden, level2_latent)
                self.enc2_logvar = nn.Linear(level2_hidden, level2_latent)
                
                # Decoder Level 2
                self.dec2_fc = nn.Sequential(
                    nn.Linear(level2_latent, level2_hidden),
                    nn.ReLU(),
                    nn.Linear(level2_hidden, level1_latent)
                )
                
                # Decoder Level 1
                self.dec1_fc = nn.Sequential(
                    nn.Linear(level1_latent, level1_hidden),
                    nn.ReLU(),
                    nn.Linear(level1_hidden, input_dim)
                )
            
            def encode(self, x):
                h1 = F.relu(self.enc1_fc(x))
                mu1 = self.enc1_mu(h1)
                logvar1 = self.enc1_logvar(h1)
                std1 = torch.exp(0.5 * logvar1)
                z1 = mu1 + std1 * torch.randn_like(std1)
                
                h2 = F.relu(self.enc2_fc(z1))
                mu2 = self.enc2_mu(h2)
                logvar2 = self.enc2_logvar(h2)
                std2 = torch.exp(0.5 * logvar2)
                z2 = mu2 + std2 * torch.randn_like(std2)
                
                return [mu1, mu2], [logvar1, logvar2], [z1, z2]
            
            def decode(self, z_list):
                z1, z2 = z_list
                z1_recon = self.dec2_fc(z2)
                x_recon = self.dec1_fc(z1_recon)
                return x_recon
            
            def forward(self, x):
                mus, logvars, z_list = self.encode(x)
                recon = self.decode(z_list)
                return recon, mus, logvars
            
            def vae_loss(self, x, recon_x, mus, logvars, reduction='mean'):
                recon_loss = F.mse_loss(recon_x, x, reduction=reduction)
                kl_loss = 0
                for mu, logvar in zip(mus, logvars):
                    level_kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)
                    if reduction == 'mean':
                        level_kl = level_kl.mean()
                    kl_loss += level_kl
                return recon_loss + self.beta * kl_loss
        
        def create_ladder_vae(input_dim, level1_latent=32, level2_latent=16, 
                              level1_hidden=128, level2_hidden=64, beta=1.0):
            return LadderVAE(input_dim, level1_latent, level2_latent, 
                           level1_hidden, level2_hidden, beta)
        # ===== END EMBEDDED LADDER VAE =====
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--num_features', type=int, default=16197)
        parser.add_argument('--level1_latent', type=int, default=32)
        parser.add_argument('--level2_latent', type=int, default=16)
        parser.add_argument('--level1_hidden', type=int, default=128)
        parser.add_argument('--level2_hidden', type=int, default=64)
        parser.add_argument('--beta', type=float, default=1.0)
        parser.add_argument('--model_path', type=str, required=True)
        parser.add_argument('--config_path', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 70)
        print("BUILDING LADDER VAE MODEL (from nesy-factory)")
        print("=" * 70)
        
        try:
            print("\\n[1/3] Creating Ladder VAE from nesy-factory...")
            
            # Create model using factory function
            model = create_ladder_vae(
                input_dim=args.num_features,
                level1_latent=args.level1_latent,
                level2_latent=args.level2_latent,
                level1_hidden=args.level1_hidden,
                level2_hidden=args.level2_hidden,
                beta=args.beta
            )
            
            # Count parameters efficiently
            total_params = sum(p.numel() for p in model.parameters())
            print(f"    Model created: {total_params:,} parameters")
            
            # Save model and config
            print("\\n[2/3] Saving checkpoint...")
            os.makedirs(args.model_path, exist_ok=True)
            os.makedirs(args.config_path, exist_ok=True)
            
            model_file = os.path.join(args.model_path, "ladder_vae_init.pt")
            torch.save(model.state_dict(), model_file)
            print(f"    Model: {os.path.getsize(model_file) / 1e6:.2f} MB")
            
            with open(os.path.join(args.model_path, "data"), "w") as f:
                f.write(model_file)
            
            print("\\n[3/3] Saving config...")
            config = {
                "model_type": "LadderVAE",
                "input_dim": args.num_features,
                "level1_latent": args.level1_latent,
                "level2_latent": args.level2_latent,
                "level1_hidden": args.level1_hidden,
                "level2_hidden": args.level2_hidden,
                "beta": args.beta,
                "total_parameters": total_params,
                "model_file": model_file
            }
            
            config_file = os.path.join(args.config_path, "model_config.json")
            with open(config_file, 'w') as f:
                json.dump(config, f, indent=2)
            print(f"    Config saved")
            
            with open(os.path.join(args.config_path, "data"), "w") as f:
                f.write(config_file)
            
            print("\\n" + "=" * 70)
            print(" LADDER VAE INITIALIZED")
            print("=" * 70)
            
        except Exception as e:
            print(f"Error building model: {str(e)}")
            import traceback
            traceback.print_exc()
            exit(1)
    
    args:
      - --num_features
      - {inputValue: num_features}
      - --level1_latent
      - {inputValue: level1_latent}
      - --level2_latent
      - {inputValue: level2_latent}
      - --level1_hidden
      - {inputValue: level1_hidden}
      - --level2_hidden
      - {inputValue: level2_hidden}
      - --beta
      - {inputValue: beta}
      - --model_path
      - {outputPath: model_path}
      - --config_path
      - {outputPath: config_path}
