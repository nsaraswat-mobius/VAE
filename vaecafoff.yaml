name: Train VAE Model Enhanced
description: Trains VAE model with Traditional CAFO or Forward Forward methods
inputs:
  - {name: model, type: Model}
  - {name: train_loader, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: trained_model, type: Model}
  - {name: epoch_loss, type: String}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v29
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        import numpy as np
        import torch
        import torch.nn as nn
        import torch.optim as optim
        
        from nesy_factory.VAE.standard_vae import StandardVAE
        from nesy_factory.VAE.beta_vae import BetaVAE
        from nesy_factory.VAE.conditional_vae import ConditionalVAE

        parser = argparse.ArgumentParser()
        parser.add_argument('--model', type=str, required=True)
        parser.add_argument('--train_loader', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--epoch_loss', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)

        with open(args.model, 'rb') as f:
            model_obj = pickle.load(f)

        with open(args.train_loader, 'rb') as f:
            train_data = pickle.load(f)
            
        if isinstance(train_data, dict) and 'loader' in train_data:
            train_loader_obj = train_data['loader']
            metadata = train_data['metadata']
            print("Loaded training data with metadata")
            print("Input dimension:", metadata['processed_input_dim'])
            print("Train samples:", metadata['train_samples'])
            print("Batch size:", metadata['batch_size'])
        else:
            train_loader_obj = train_data
            metadata = None
            print("Loaded training data legacy format without metadata")

        print("Starting VAE Model Training")
        epoch_loss_data = []

        if not hasattr(model_obj, 'use_cafo'):
            model_obj.use_cafo = False
            print("Backward compatibility: Setting use_cafo=False for existing model")
        
        if not hasattr(model_obj, 'use_forward_forward'):
            model_obj.use_forward_forward = False
            print("Backward compatibility: Setting use_forward_forward=False for existing model")

        use_cafo_from_config = config.get('use_cafo', False)
        use_ff_from_config = config.get('use_forward_forward', False)
        
        if sum([use_cafo_from_config, use_ff_from_config]) > 1:
            raise ValueError("Only one training method can be selected: use_cafo or use_forward_forward")
        
        if use_cafo_from_config and not model_obj.use_cafo:
            print("Warning: Config requests CAFO but model was created without CAFO support.")
            print("Creating new CAFO VAE model with same architecture...")
            
            # Handle CAFO configuration based on user preference
            requested_cafo_blocks = config.get('cafo_blocks', len(model_obj.hidden_dims))
            current_hidden_dims = model_obj.hidden_dims
            
            print(f"ðŸŽ¯ USER WANTS: {requested_cafo_blocks} CAFO blocks")
            print(f"ðŸ“ CURRENT ARCHITECTURE: {current_hidden_dims} (supports {len(current_hidden_dims)} blocks)")
            
            # If user wants more blocks than current architecture supports, create a suitable architecture
            if requested_cafo_blocks > len(current_hidden_dims):
                print(f"ðŸ”§ EXPANDING ARCHITECTURE: Creating {requested_cafo_blocks} blocks")
                
                # Create progressive dimension reduction for requested blocks
                start_dim = current_hidden_dims[0] if current_hidden_dims else 512
                end_dim = model_obj.latent_dim
                
                # Generate progressive dimensions
                new_hidden_dims = []
                for i in range(requested_cafo_blocks):
                    # Progressive reduction: start_dim -> end_dim over requested_cafo_blocks steps
                    ratio = (requested_cafo_blocks - i - 1) / (requested_cafo_blocks - 1) if requested_cafo_blocks > 1 else 0
                    dim = int(start_dim * ratio + end_dim * (1 - ratio))
                    dim = max(dim, end_dim)  # Don't go below latent_dim
                    new_hidden_dims.append(dim)
                
                print(f"ðŸ“ˆ NEW ARCHITECTURE: {new_hidden_dims}")
                use_hidden_dims = new_hidden_dims
            else:
                print(f"âœ… USING EXISTING ARCHITECTURE: {current_hidden_dims}")
                use_hidden_dims = current_hidden_dims
                requested_cafo_blocks = len(current_hidden_dims)  # Match architecture
            
            new_config = {
                'input_dim': model_obj.input_dim,
                'latent_dim': model_obj.latent_dim,
                'hidden_dims': use_hidden_dims,
                'learning_rate': config.get('learning_rate', 0.001),
                'beta': config.get('beta', 1.0),
                'use_cafo': True,
                'cafo_blocks': requested_cafo_blocks,
                'epochs_per_block': config.get('epochs_per_block', 50),
                'block_lr': config.get('block_lr', 0.001)
            }
            
            print("ðŸ”§ FINAL CAFO CONFIG:")
            for key, value in new_config.items():
                print(f"  {key}: {value}")
            
            print("ðŸŽ¯ EXPECTED CAFO ARCHITECTURE:")
            print(f"  Input: {new_config['input_dim']}")
            for i, dim in enumerate(new_config['hidden_dims']):
                print(f"  Block {i+1}: â†’ {dim}")
            print(f"  Output: {new_config['latent_dim']}")
            print(f"  Total blocks needed: {len(new_config['hidden_dims'])}")
            
            model_obj = StandardVAE(new_config)
            print("Created new CAFO VAE model with", new_config['cafo_blocks'], "blocks")
        
        elif use_ff_from_config and not model_obj.use_forward_forward:
            print("Warning: Config requests Forward Forward but model was created without FF support.")
            print("Creating new Forward Forward VAE model with same architecture...")
            
            new_config = {
                'input_dim': model_obj.input_dim,
                'latent_dim': model_obj.latent_dim,
                'hidden_dims': model_obj.hidden_dims,
                'learning_rate': config.get('learning_rate', 0.001),
                'beta': config.get('beta', 1.0),
                'use_forward_forward': True,
                'ff_blocks': config.get('ff_blocks', 3),
                'ff_threshold': config.get('ff_threshold', 2.0),
                'ff_epochs_per_block': config.get('ff_epochs_per_block', 100),
                'ff_lr': config.get('ff_lr', 0.03)
            }
            
            model_obj = StandardVAE(new_config)
            print("Created new Forward Forward VAE model with", new_config['ff_blocks'], "blocks")

        X_train_list = []
        for batch_data in train_loader_obj:
            if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 1:
                data = batch_data[0]
            else:
                data = batch_data
            X_train_list.append(data)
        
        X_train = torch.cat(X_train_list, dim=0)
        print("Training data shape: X=", X_train.shape)

        learning_rate = config.get('learning_rate', 0.001)
        epochs = config.get('epochs', 100)
        beta = config.get('beta', 1.0)
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model_obj = model_obj.to(device)
        X_train = X_train.to(device)
        
        print("Training VAE on device:", device)
        print("Training configuration:")
        print("Learning rate:", learning_rate)
        print("Epochs:", epochs)
        print("Beta KL weight:", beta)

        print("Validating model architecture against data dimensions...")
        
        if hasattr(X_train, 'shape'):
            actual_input_dim = X_train.shape[1]
            print("Actual data input dimension:", actual_input_dim)
        else:
            print("Error: X_train is not a tensor. Type:", type(X_train))
            raise ValueError("Expected tensor data but got " + str(type(X_train)))
        
        if not (hasattr(model_obj, 'use_cafo') and model_obj.use_cafo) and not (hasattr(model_obj, 'use_forward_forward') and model_obj.use_forward_forward):
            try:
                model_obj.eval()
                with torch.no_grad():
                    test_output = model_obj(X_train[:1])
                    print("Model output type:", type(test_output))
                    if isinstance(test_output, tuple):
                        print("Model output length:", len(test_output))
                print("Model architecture validation: PASSED")
                model_obj.train()
            except RuntimeError as e:
                if "cannot be multiplied" in str(e):
                    raise RuntimeError("Model input dimension mismatch: " + str(e))
                else:
                    raise e

        if hasattr(model_obj, 'use_forward_forward') and model_obj.use_forward_forward:
            print("Using Forward Forward training mode")
            print("FF blocks:", getattr(model_obj, 'ff_blocks', 'unknown'))
            print("FF threshold:", getattr(model_obj, 'ff_threshold', 'unknown'))
            print("Epochs per FF block:", getattr(model_obj, 'ff_epochs_per_block', 'unknown'))
            print("FF learning rate:", getattr(model_obj, 'ff_lr', 'unknown'))
            
            try:
                print("Starting Forward Forward training with enhanced error handling...")
                ff_results = model_obj.train_forward_forward(X_train, verbose=True)
            except Exception as e:
                print("ERROR in Forward Forward training:", str(e))
                print("Falling back to traditional VAE training...")
                
                # Create a new traditional VAE model for fallback
                print("Creating new traditional VAE model for fallback...")
                
                fallback_config = {
                    'input_dim': model_obj.input_dim,
                    'latent_dim': model_obj.latent_dim,
                    'hidden_dims': getattr(model_obj, 'hidden_dims', [512, 256]),
                    'learning_rate': learning_rate,
                    'beta': beta,
                    'use_cafo': False,
                    'use_forward_forward': False
                }
                
                # Create new traditional model
                model_obj = StandardVAE(fallback_config).to(device)
                optimizer = optim.Adam(model_obj.parameters(), lr=learning_rate)
                
                print("Training for", epochs, "epochs using traditional method")
                
                model_obj.train()
                for epoch in range(epochs):
                    total_loss = 0
                    total_recon_loss = 0
                    total_kl_loss = 0
                    num_batches = 0
                    
                    for batch_data in train_loader_obj:
                        if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 1:
                            data = batch_data[0]
                        else:
                            data = batch_data
                        
                        data = data.to(device)
                        optimizer.zero_grad()
                        
                        try:
                            vae_output = model_obj(data)
                            
                            if isinstance(vae_output, tuple) and len(vae_output) == 3:
                                recon_data, mu, logvar = vae_output
                            elif isinstance(vae_output, dict):
                                recon_data = vae_output.get('reconstruction', vae_output.get('recon', data))
                                mu = vae_output.get('mu', torch.zeros(data.size(0), model_obj.latent_dim).to(device))
                                logvar = vae_output.get('logvar', torch.zeros(data.size(0), model_obj.latent_dim).to(device))
                            else:
                                recon_data = vae_output
                                mu = torch.zeros(data.size(0), model_obj.latent_dim).to(device)
                                logvar = torch.zeros(data.size(0), model_obj.latent_dim).to(device)
                            
                            recon_loss = nn.MSELoss()(recon_data, data)
                            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                            kl_loss = kl_loss / data.size(0)
                            
                            loss = recon_loss + beta * kl_loss
                            
                            loss.backward()
                            optimizer.step()
                            
                            total_loss += loss.item()
                            total_recon_loss += recon_loss.item()
                            total_kl_loss += kl_loss.item()
                            num_batches += 1
                            
                        except Exception as inner_e:
                            print("Error in fallback training batch:", str(inner_e))
                            continue
                    
                    if num_batches > 0:
                        avg_loss = total_loss / num_batches
                        avg_recon_loss = total_recon_loss / num_batches
                        avg_kl_loss = total_kl_loss / num_batches
                        
                        print("Epoch", epoch+1, "/", epochs, "| Total Loss:", avg_loss, "| Recon Loss:", avg_recon_loss, "| KL Loss:", avg_kl_loss)
                        
                        epoch_loss_data.append({
                            'epoch': epoch + 1,
                            'loss': avg_loss,
                            'accuracy': 0.0,
                            'validation_loss': avg_loss,
                            'validation_accuracy': 0.0,
                            'custom_metrics': {
                                'training_mode': 'traditional_fallback',
                                'recon_loss': avg_recon_loss,
                                'kl_loss': avg_kl_loss,
                                'total_loss': avg_loss,
                                'ff_error': str(e)[:100]  # First 100 chars of error
                            }
                        })
                    
                    if epoch > 10 and avg_loss > 1000:
                        print("Warning: Loss is diverging, stopping training early")
                        break
                
                print("Fallback training completed successfully")
                ff_results = None  # Set to None to skip FF results processing
            
            # Only process FF results if training was successful
            if ff_results is not None:
                for i, block_result in enumerate(ff_results['encoder_results']):
                    for epoch, loss in enumerate(block_result['losses']):
                        epoch_loss_data.append({
                            'epoch': epoch + 1,
                            'loss': float(loss),
                            'accuracy': 0.0,
                            'validation_loss': float(loss),
                            'validation_accuracy': 0.0,
                            'custom_metrics': {
                                'block': i + 1,
                                'block_type': 'encoder',
                                'training_mode': 'forward_forward',
                                'recon_loss': 0.0,
                                'kl_loss': 0.0,
                                'total_loss': float(loss)
                            }
                        })
                
                for i, block_result in enumerate(ff_results['decoder_results']):
                    for epoch, loss in enumerate(block_result['losses']):
                        epoch_loss_data.append({
                            'epoch': epoch + 1,
                            'loss': float(loss),
                            'accuracy': 0.0,
                            'validation_loss': float(loss),
                            'validation_accuracy': 0.0,
                            'custom_metrics': {
                                'block': i + 1,
                                'block_type': 'decoder',
                                'training_mode': 'forward_forward',
                                'recon_loss': 0.0,
                                'kl_loss': 0.0,
                                'total_loss': float(loss)
                            }
                        })
                
                print("Forward Forward training completed in", ff_results['total_training_time'], "seconds")
                print("Total encoder blocks trained:", len(ff_results['encoder_results']))
                print("Total decoder blocks trained:", len(ff_results['decoder_results']))
            else:
                print("Forward Forward training failed, used fallback traditional training")
            
        elif hasattr(model_obj, 'use_cafo') and model_obj.use_cafo:
            print("Using CAFO Cascaded Forward training mode")
            print("CAFO blocks:", getattr(model_obj, 'cafo_blocks', 'unknown'))
            print("Epochs per block:", getattr(model_obj, 'epochs_per_block', 'unknown'))
            
            try:
                # Add detailed debugging for CAFO training
                print("Starting CAFO training with enhanced error handling and debugging...")
                print(" DEBUGGING CAFO ARCHITECTURE:")
                print(f"  Model input_dim: {model_obj.input_dim}")
                print(f"  Model latent_dim: {model_obj.latent_dim}")
                print(f"  Model hidden_dims: {getattr(model_obj, 'hidden_dims', 'Not found')}")
                print(f"  CAFO blocks count: {getattr(model_obj, 'cafo_blocks', 'Not found')}")
                print(f"  Training data shape: {X_train.shape}")
                
                # Check if CAFO blocks exist and their configuration
                if hasattr(model_obj, 'encoder_blocks'):
                    print(f"  Number of encoder blocks: {len(model_obj.encoder_blocks)}")
                    for i, block in enumerate(model_obj.encoder_blocks):
                        print(f"    Encoder Block {i+1}: {block}")
                        if hasattr(block, 'input_dim') and hasattr(block, 'output_dim'):
                            print(f"      Input dim: {block.input_dim}, Output dim: {block.output_dim}")
                else:
                    print("   No encoder_blocks found!")
                
                if hasattr(model_obj, 'decoder_blocks'):
                    print(f"  Number of decoder blocks: {len(model_obj.decoder_blocks)}")
                    for i, block in enumerate(model_obj.decoder_blocks):
                        print(f"    Decoder Block {i+1}: {block}")
                        if hasattr(block, 'input_dim') and hasattr(block, 'output_dim'):
                            print(f"      Input dim: {block.input_dim}, Output dim: {block.output_dim}")
                else:
                    print("   No decoder_blocks found!")
                
                print(" Starting CAFO training...")
                cafo_results = model_obj.train_cafo(X_train, verbose=True)
            except RuntimeError as e:
                if "size of tensor" in str(e) and "must match the size" in str(e):
                    print(" ERROR: Tensor dimension mismatch in CAFO training")
                    print(f" Full error message: {str(e)}")
                    print(" DEBUGGING TENSOR DIMENSIONS:")
                    
                    # Extract dimension information from error message
                    import re
                    size_match = re.search(r'size of tensor a \((\d+)\) must match the size of tensor b \((\d+)\)', str(e))
                    if size_match:
                        dim_a, dim_b = size_match.groups()
                        print(f"  Tensor A dimension: {dim_a}")
                        print(f"  Tensor B dimension: {dim_b}")
                        print(f"  Expected flow: 21 â†’ 512 â†’ 256 â†’ 8")
                        print(f"  Actual conflict: {dim_a} vs {dim_b}")
                    
                    # Try to inspect the current model state
                    try:
                        if hasattr(model_obj, 'encoder_blocks') and len(model_obj.encoder_blocks) > 1:
                            print("ðŸ”§ ENCODER BLOCK ANALYSIS:")
                            for i, block in enumerate(model_obj.encoder_blocks):
                                print(f"  Block {i+1}: {type(block).__name__}")
                                if hasattr(block, 'layers'):
                                    for j, layer in enumerate(block.layers):
                                        if hasattr(layer, 'in_features') and hasattr(layer, 'out_features'):
                                            print(f"    Layer {j+1}: {layer.in_features} â†’ {layer.out_features}")
                    except Exception as inspect_error:
                        print(f"  Could not inspect model blocks: {inspect_error}")
                    
                    print("This is likely due to architecture incompatibility")
                    print("Falling back to traditional VAE training...")
                    
                    # Create a new traditional VAE model for fallback
                    print("Creating new traditional VAE model for fallback...")
                    
                    fallback_config = {
                        'input_dim': model_obj.input_dim,
                        'latent_dim': model_obj.latent_dim,
                        'hidden_dims': getattr(model_obj, 'hidden_dims', [512, 256]),
                        'learning_rate': learning_rate,
                        'beta': beta,
                        'use_cafo': False,
                        'use_forward_forward': False
                    }
                    
                    # Create new traditional model
                    model_obj = StandardVAE(fallback_config).to(device)
                    optimizer = optim.Adam(model_obj.parameters(), lr=learning_rate)
                    
                    print("Training for", epochs, "epochs using traditional method")
                    
                    model_obj.train()
                    for epoch in range(epochs):
                        total_loss = 0
                        total_recon_loss = 0
                        total_kl_loss = 0
                        num_batches = 0
                        
                        for batch_data in train_loader_obj:
                            if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 1:
                                data = batch_data[0]
                            else:
                                data = batch_data
                            
                            data = data.to(device)
                            optimizer.zero_grad()
                            
                            try:
                                vae_output = model_obj(data)
                                
                                if isinstance(vae_output, tuple) and len(vae_output) == 3:
                                    recon_data, mu, logvar = vae_output
                                elif isinstance(vae_output, dict):
                                    recon_data = vae_output.get('reconstruction', vae_output.get('recon', data))
                                    mu = vae_output.get('mu', torch.zeros(data.size(0), model_obj.latent_dim).to(device))
                                    logvar = vae_output.get('logvar', torch.zeros(data.size(0), model_obj.latent_dim).to(device))
                                else:
                                    recon_data = vae_output
                                    mu = torch.zeros(data.size(0), model_obj.latent_dim).to(device)
                                    logvar = torch.zeros(data.size(0), model_obj.latent_dim).to(device)
                                
                                recon_loss = nn.MSELoss()(recon_data, data)
                                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                                kl_loss = kl_loss / data.size(0)
                                
                                loss = recon_loss + beta * kl_loss
                                
                                loss.backward()
                                optimizer.step()
                                
                                total_loss += loss.item()
                                total_recon_loss += recon_loss.item()
                                total_kl_loss += kl_loss.item()
                                num_batches += 1
                                
                            except Exception as inner_e:
                                print("Error in fallback training batch:", str(inner_e))
                                continue
                        
                        if num_batches > 0:
                            avg_loss = total_loss / num_batches
                            avg_recon_loss = total_recon_loss / num_batches
                            avg_kl_loss = total_kl_loss / num_batches
                            
                            print("Epoch", epoch+1, "/", epochs, "| Total Loss:", avg_loss, "| Recon Loss:", avg_recon_loss, "| KL Loss:", avg_kl_loss)
                            
                            epoch_loss_data.append({
                                'epoch': epoch + 1,
                                'loss': avg_loss,
                                'accuracy': 0.0,
                                'validation_loss': avg_loss,
                                'validation_accuracy': 0.0,
                                'custom_metrics': {
                                    'training_mode': 'traditional_fallback',
                                    'recon_loss': avg_recon_loss,
                                    'kl_loss': avg_kl_loss,
                                    'total_loss': avg_loss,
                                    'cafo_error': 'tensor_dimension_mismatch'
                                }
                            })
                        
                        if epoch > 10 and avg_loss > 1000:
                            print("Warning: Loss is diverging, stopping training early")
                            break
                    
                    print("Fallback training completed successfully")
                    cafo_results = None  # Set to None to skip CAFO results processing
                else:
                    raise e  # Re-raise if it's a different error
            
            # Only process CAFO results if training was successful
            if cafo_results is not None:
                for i, block_result in enumerate(cafo_results['encoder_results']):
                    for epoch, loss in enumerate(block_result['train_losses']):
                        epoch_loss_data.append({
                            'epoch': epoch + 1,
                            'loss': float(loss),
                            'accuracy': 0.0,
                            'validation_loss': float(block_result.get('val_losses', [loss])[epoch] if epoch < len(block_result.get('val_losses', [])) else loss),
                            'validation_accuracy': 0.0,
                            'custom_metrics': {
                                'block': i + 1,
                                'block_type': 'encoder',
                                'training_mode': 'cafo',
                                'recon_loss': 0.0,
                                'kl_loss': 0.0,
                                'total_loss': float(loss)
                            }
                        })
                
                for i, block_result in enumerate(cafo_results['decoder_results']):
                    for epoch, loss in enumerate(block_result['train_losses']):
                        epoch_loss_data.append({
                            'epoch': epoch + 1,
                            'loss': float(loss),
                            'accuracy': 0.0,
                            'validation_loss': float(block_result.get('val_losses', [loss])[epoch] if epoch < len(block_result.get('val_losses', [])) else loss),
                            'validation_accuracy': 0.0,
                            'custom_metrics': {
                                'block': i + 1,
                                'block_type': 'decoder',
                                'training_mode': 'cafo',
                                'recon_loss': 0.0,
                                'kl_loss': 0.0,
                                'total_loss': float(loss)
                            }
                        })
                
                print("CAFO training completed in", cafo_results['total_training_time'], "seconds")
                print("Total encoder blocks trained:", len(cafo_results['encoder_results']))
                print("Total decoder blocks trained:", len(cafo_results['decoder_results']))
            else:
                print("CAFO training failed, used fallback traditional training")
            
        else:
            print("Using traditional VAE training mode")
            
            optimizer = optim.Adam(model_obj.parameters(), lr=learning_rate)
            
            print("Training for", epochs, "epochs")
            
            model_obj.train()
            for epoch in range(epochs):
                total_loss = 0
                total_recon_loss = 0
                total_kl_loss = 0
                num_batches = 0
                
                for batch_data in train_loader_obj:
                    if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 1:
                        data = batch_data[0]
                    else:
                        data = batch_data
                    
                    data = data.to(device)
                    optimizer.zero_grad()
                    
                    vae_output = model_obj(data)
                    
                    if isinstance(vae_output, dict):
                        possible_recon_keys = ['reconstruction', 'recon', 'x_recon', 'decoded', 'output']
                        possible_mu_keys = ['mu', 'mean', 'latent_mean', 'z_mean']
                        possible_logvar_keys = ['logvar', 'log_var', 'latent_logvar', 'z_logvar', 'log_variance']
                        
                        recon_data = None
                        for key in possible_recon_keys:
                            if key in vae_output:
                                recon_data = vae_output[key]
                                break
                        
                        mu = None
                        for key in possible_mu_keys:
                            if key in vae_output:
                                mu = vae_output[key]
                                break
                        
                        logvar = None
                        for key in possible_logvar_keys:
                            if key in vae_output:
                                logvar = vae_output[key]
                                break
                        
                        if recon_data is None:
                            recon_data = list(vae_output.values())[0] if vae_output else data
                        if mu is None:
                            mu = torch.zeros(data.size(0), getattr(model_obj, 'latent_dim', 16)).to(device)
                        if logvar is None:
                            logvar = torch.zeros(data.size(0), getattr(model_obj, 'latent_dim', 16)).to(device)
                            
                    elif isinstance(vae_output, tuple):
                        if len(vae_output) == 3:
                            recon_data, mu, logvar = vae_output
                        else:
                            recon_data = vae_output[0]
                            mu = vae_output[1] if len(vae_output) > 1 else torch.zeros(data.size(0), getattr(model_obj, 'latent_dim', 16)).to(device)
                            logvar = vae_output[2] if len(vae_output) > 2 else torch.zeros(data.size(0), getattr(model_obj, 'latent_dim', 16)).to(device)
                    else:
                        recon_data = vae_output
                        mu = torch.zeros(data.size(0), getattr(model_obj, 'latent_dim', 16)).to(device)
                        logvar = torch.zeros(data.size(0), getattr(model_obj, 'latent_dim', 16)).to(device)
                    
                    recon_loss = nn.MSELoss()(recon_data, data)
                    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                    kl_loss = kl_loss / data.size(0)
                    
                    loss = recon_loss + beta * kl_loss
                    
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                    total_recon_loss += recon_loss.item()
                    total_kl_loss += kl_loss.item()
                    num_batches += 1
                
                if num_batches > 0:
                    avg_loss = total_loss / num_batches
                    avg_recon_loss = total_recon_loss / num_batches
                    avg_kl_loss = total_kl_loss / num_batches
                    
                    print("Epoch", epoch+1, "/", epochs, "| Total Loss:", avg_loss, "| Recon Loss:", avg_recon_loss, "| KL Loss:", avg_kl_loss)
                    
                    epoch_loss_data.append({
                        'epoch': epoch + 1,
                        'loss': avg_loss,
                        'accuracy': 0.0,
                        'validation_loss': avg_loss,
                        'validation_accuracy': 0.0,
                        'custom_metrics': {
                            'training_mode': 'traditional',
                            'recon_loss': avg_recon_loss,
                            'kl_loss': avg_kl_loss,
                            'total_loss': avg_loss
                        }
                    })
                
                if epoch > 10 and avg_loss > 1000:
                    print("Warning: Loss is diverging, stopping training early")
                    break

        # Determine actual training mode used (including fallbacks)
        if epoch_loss_data:
            last_custom_metrics = epoch_loss_data[-1].get('custom_metrics', {})
            training_mode = last_custom_metrics.get('training_mode', 'traditional')
        else:
            training_mode = 'forward_forward' if (hasattr(model_obj, 'use_forward_forward') and model_obj.use_forward_forward) else 'cafo' if (hasattr(model_obj, 'use_cafo') and model_obj.use_cafo) else 'traditional'
        
        print("VAE Model Training Completed - Mode:", training_mode.upper())
        print("Loss entries recorded:", len(epoch_loss_data))

        output_dir_epoch_loss = os.path.dirname(args.epoch_loss)
        if output_dir_epoch_loss and not os.path.exists(output_dir_epoch_loss):
            os.makedirs(output_dir_epoch_loss, exist_ok=True)

        training_summary = {
            'training_mode': training_mode,
            'total_loss_entries': len(epoch_loss_data),
            'final_loss': epoch_loss_data[-1]['loss'] if epoch_loss_data else 0.0,
            'model_config': {
                'input_dim': model_obj.input_dim,
                'latent_dim': model_obj.latent_dim,
                'training_method': training_mode
            },
            'loss_history': epoch_loss_data,
            "total_epochs": len(epoch_loss_data),
            "final_total_loss": epoch_loss_data[-1]["custom_metrics"]["total_loss"] if epoch_loss_data and "custom_metrics" in epoch_loss_data[-1] else (epoch_loss_data[-1]["loss"] if epoch_loss_data else 0.0),
            "final_recon_loss": epoch_loss_data[-1]["custom_metrics"].get("recon_loss", 0.0) if epoch_loss_data and "custom_metrics" in epoch_loss_data[-1] else 0.0,
            "final_kl_loss": epoch_loss_data[-1]["custom_metrics"].get("kl_loss", 0.0) if epoch_loss_data and "custom_metrics" in epoch_loss_data[-1] else 0.0,
            "epoch_losses": epoch_loss_data,
            "convergence_achieved": (epoch_loss_data[-1]["custom_metrics"]["total_loss"] if epoch_loss_data and "custom_metrics" in epoch_loss_data[-1] else epoch_loss_data[-1]["loss"] if epoch_loss_data else float('inf')) < 1.0,
            "training_config": {
                "learning_rate": learning_rate,
                "beta": beta,
                "optimizer": "Adam" if training_mode == 'traditional' else training_mode
            }
        }

        with open(args.epoch_loss, 'w') as f:
            json.dump(training_summary, f, indent=2)
        
        print("Training Summary: Final Loss =", training_summary['final_total_loss'])
        print("Convergence:", 'Yes' if training_summary['convergence_achieved'] else 'No')
        
        output_dir_trained_model = os.path.dirname(args.trained_model)
        if output_dir_trained_model and not os.path.exists(output_dir_trained_model):
            os.makedirs(output_dir_trained_model, exist_ok=True)
        with open(args.trained_model, 'wb') as f:
            pickle.dump(model_obj.cpu(), f)
        
        print("Saved trained VAE model to", args.trained_model)
        print("Saved training summary to", args.epoch_loss)
        print("Training method used:", training_mode.upper())
        
        total_params = sum(p.numel() for p in model_obj.parameters())
        trainable_params = sum(p.numel() for p in model_obj.parameters() if p.requires_grad)
        print("Model statistics:")
        print("Total parameters:", total_params)
        print("Trainable parameters:", trainable_params)
        print("Training method:", training_mode)
    args:
      - --model
      - {inputPath: model}
      - --train_loader
      - {inputPath: train_loader}
      - --config
      - {inputValue: config}
      - --trained_model
      - {outputPath: trained_model}
      - --epoch_loss
      - {outputPath: epoch_loss}
