name: Train VAE Model 4
description: Trains the VAE model with Traditional, CAFO, or Forward Forward methods.
inputs:
  - {name: model, type: Model}
  - {name: train_loader, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: trained_model, type: Model}
  - {name: epoch_loss, type: String}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v30
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        import torch
        import torch.nn as nn
        import torch.optim as optim
        import uuid
        import numpy as np
        
        from nesy_factory.VAE.standard_vae import StandardVAE

        parser = argparse.ArgumentParser()
        parser.add_argument('--model', type=str, required=True)
        parser.add_argument('--train_loader', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--epoch_loss', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)

        with open(args.model, 'rb') as f:
            model_obj = pickle.load(f)

        with open(args.train_loader, 'rb') as f:
            train_data = pickle.load(f)
            
        if isinstance(train_data, dict) and 'loader' in train_data:
            train_loader_obj = train_data['loader']
            metadata = train_data['metadata']
            print("Loaded training data with metadata")
        else:
            train_loader_obj = train_data
            metadata = None
            print("Loaded training data legacy format")

        print("Starting VAE Model Training")
        epoch_loss_data = []

        # Handle backward compatibility for old models
        if not hasattr(model_obj, 'use_cafo'):
            model_obj.use_cafo = False
            print("Backward compatibility: Setting use_cafo=False for existing model")
        
        if not hasattr(model_obj, 'use_forward_forward'):
            model_obj.use_forward_forward = False
            print("Backward compatibility: Setting use_forward_forward=False for existing model")

        # Check training method from config
        use_cafo_from_config = config.get('use_cafo', False)
        use_ff_from_config = config.get('use_forward_forward', False)
        
        # Validate only one training method is selected
        if sum([use_cafo_from_config, use_ff_from_config]) > 1:
            raise ValueError("Only one training method can be selected: use_cafo or use_forward_forward")
        
        # Extract training data to determine actual input dimensions
        X_train_list = []
        for batch_data in train_loader_obj:
            if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 1:
                data = batch_data[0]
            else:
                data = batch_data
            X_train_list.append(data)
        
        X_train = torch.cat(X_train_list, dim=0)
        actual_input_dim = X_train.shape[1]
        print(f"Training data shape: X={X_train.shape}")
        print(f"Actual input dimension: {actual_input_dim}")
        
        # Handle CAFO model creation/compatibility
        if use_cafo_from_config and not model_obj.use_cafo:
            print("Warning: Config requests CAFO but model was created without CAFO support.")
            print("Creating new CAFO model with same architecture...")
            
            # Create architecture for requested CAFO blocks
            requested_cafo_blocks = config.get('cafo_blocks', len(model_obj.hidden_dims))
            
            # Generate progressive dimensions for CAFO blocks
            start_dim = actual_input_dim
            end_dim = model_obj.latent_dim
            
            if requested_cafo_blocks == 1:
                import math
                hidden_dim = int(math.sqrt(start_dim * end_dim))
                if hidden_dim == start_dim or hidden_dim == end_dim:
                    hidden_dim = int((start_dim + end_dim) / 2)
                new_hidden_dims = [hidden_dim]
            elif requested_cafo_blocks == 2:
                import math
                ratio = (end_dim / start_dim) ** (1.0 / 3)
                dim1 = int(start_dim * ratio)
                dim2 = int(start_dim * ratio * ratio)
                dim1 = max(dim1, end_dim + 3)
                dim2 = max(dim2, end_dim + 1)
                if dim1 <= dim2:
                    dim1 = dim2 + 2
                new_hidden_dims = [dim1, dim2]
            else:
                new_hidden_dims = []
                step = (start_dim - end_dim) / (requested_cafo_blocks + 1)
                for i in range(requested_cafo_blocks):
                    dim = int(start_dim - step * (i + 1))
                    dim = max(dim, end_dim)
                    new_hidden_dims.append(dim)
            
            new_config = {
                'input_dim': actual_input_dim,
                'latent_dim': model_obj.latent_dim,
                'hidden_dims': new_hidden_dims,
                'learning_rate': config.get('learning_rate', 0.001),
                'beta': config.get('beta', 1.0),
                'use_cafo': True,
                'cafo_blocks': requested_cafo_blocks,
                'epochs_per_block': config.get('epochs_per_block', 50),
                'block_lr': config.get('block_lr', 0.001)
            }
            
            model_obj = StandardVAE(new_config)
            print(f"Created new CAFO model with {new_config['cafo_blocks']} blocks")
        
        # Handle Forward Forward model creation/compatibility
        elif use_ff_from_config and not model_obj.use_forward_forward:
            print("Creating new Forward Forward VAE model with working architecture...")
            
            # Get FF parameters
            ff_blocks = config.get('ff_blocks', 2)
            ff_epochs_per_block = config.get('ff_epochs_per_block', 20)
            
            # CRITICAL FIX: Create MUCH simpler architecture
            # Forward Forward works best with small, simple networks
            if ff_blocks == 1:
                hidden_dims = [32]
            elif ff_blocks == 2:
                hidden_dims = [48, 32]
            else:
                hidden_dims = [64, 48, 32]
                ff_blocks = 3
            
            # Get latent dim from config or use default
            latent_dim = config.get('latent_dim', 8)  # Smaller latent space for FF
            
            new_config = {
                'input_dim': actual_input_dim,
                'latent_dim': latent_dim,
                'hidden_dims': hidden_dims,
                'learning_rate': config.get('learning_rate', 0.001),
                'beta': 0.1,  # VERY LOW beta for FF
                'use_forward_forward': True,
                'use_cafo': False,
                'ff_blocks': ff_blocks,
                'ff_epochs_per_block': ff_epochs_per_block,
                'ff_lr': 0.005,  # Lower learning rate
                'ff_threshold': 0.5  # Lower threshold
            }
            
            model_obj = StandardVAE(new_config)
            print(f"Created new Forward Forward model with {new_config['ff_blocks']} blocks")
            print(f"FF Architecture: {actual_input_dim} -> {hidden_dims} -> {latent_dim}")
            print(f"Simple FF architecture to prevent NaN")
        
        # Update model input dimension if needed
        if hasattr(model_obj, 'input_dim') and actual_input_dim != model_obj.input_dim:
            print(f"Updating model input_dim from {model_obj.input_dim} to {actual_input_dim}")
            model_obj.input_dim = actual_input_dim

        # Move to device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model_obj = model_obj.to(device)
        X_train = X_train.to(device)
        
        print(f"Training VAE on device: {device}")

        # Training logic based on method
        if hasattr(model_obj, 'use_forward_forward') and model_obj.use_forward_forward:
            print("Using Forward Forward training mode")
            
            # CRITICAL FIX: Replace the broken train_forward_forward with our own implementation
            print("Using custom Forward Forward training implementation...")
            
            # Normalize data
            X_mean = X_train.mean(dim=0, keepdim=True)
            X_std = X_train.std(dim=0, keepdim=True) + 1e-8
            X_train_normalized = (X_train - X_mean) / X_std
            
            # Get parameters
            ff_blocks = getattr(model_obj, 'ff_blocks', 2)
            ff_epochs_per_block = getattr(model_obj, 'ff_epochs_per_block', 20)
            ff_lr = getattr(model_obj, 'ff_lr', 0.005)
            
            # Initialize with very small weights
            for param in model_obj.parameters():
                if param.dim() > 1:
                    nn.init.normal_(param, mean=0.0, std=0.01)
                else:
                    nn.init.constant_(param, 0.01)
            
            # Custom FF training implementation
            def custom_ff_training(model, X, blocks=2, epochs_per_block=20, lr=0.005):
                results = {
                    'encoder_results': [],
                    'decoder_results': [],
                    'total_training_time': 0
                }
                
                model.train()
                batch_size = min(64, len(X))
                n_batches = len(X) // batch_size
                
                # Create positive and negative samples
                pos_data = X
                neg_data = X[torch.randperm(len(X))]
                
                # Train encoder blocks
                for block_idx in range(blocks):
                    print(f"--- Training Encoder Block {block_idx+1}/{blocks} ---")
                    block_losses = []
                    
                    # Get the specific layer to train
                    if block_idx < len(model.encoder):
                        layer = model.encoder[block_idx * 2]  # Linear layer
                    else:
                        continue
                    
                    optimizer = torch.optim.SGD([layer.weight, layer.bias], lr=lr)
                    
                    for epoch in range(epochs_per_block):
                        epoch_loss = 0
                        
                        for batch in range(n_batches):
                            start = batch * batch_size
                            end = start + batch_size
                            
                            pos_batch = pos_data[start:end]
                            neg_batch = neg_data[start:end]
                            
                            optimizer.zero_grad()
                            
                            # Forward pass
                            pos_out = layer(pos_batch)
                            neg_out = layer(neg_batch)
                            
                            # Goodness score: norm of activations
                            pos_goodness = torch.norm(pos_out, dim=1).mean()
                            neg_goodness = torch.norm(neg_out, dim=1).mean()
                            
                            # Loss: maximize positive goodness, minimize negative
                            loss = -torch.log(torch.sigmoid(pos_goodness - 0.5)) + torch.log(torch.sigmoid(neg_goodness - 0.5))
                            
                            # Avoid NaN
                            if torch.isnan(loss):
                                loss = torch.tensor(0.0, device=X.device)
                            
                            loss.backward()
                            torch.nn.utils.clip_grad_norm_([layer.weight, layer.bias], max_norm=1.0)
                            optimizer.step()
                            
                            epoch_loss += loss.item()
                        
                        avg_loss = epoch_loss / max(n_batches, 1)
                        block_losses.append(avg_loss)
                        
                        if (epoch + 1) % 5 == 0 or epoch == 0:
                            print(f"    Epoch {epoch+1}/{epochs_per_block} - Loss: {avg_loss:.6f}")
                    
                    results['encoder_results'].append({'losses': block_losses})
                
                # Train decoder blocks similarly (simplified)
                for block_idx in range(blocks):
                    print(f"--- Training Decoder Block {block_idx+1}/{blocks} ---")
                    block_losses = []
                    
                    if block_idx < len(model.decoder):
                        layer = model.decoder[block_idx * 2]
                    else:
                        continue
                    
                    optimizer = torch.optim.SGD([layer.weight, layer.bias], lr=lr)
                    
                    for epoch in range(epochs_per_block):
                        epoch_loss = 0
                        
                        # Sample random latent vectors
                        z = torch.randn(batch_size, model.latent_dim, device=X.device)
                        
                        for batch in range(n_batches):
                            optimizer.zero_grad()
                            
                            # Forward pass through decoder
                            recon = model.decode(z)
                            
                            # Simple reconstruction loss
                            loss = nn.MSELoss()(recon, torch.randn_like(recon) * 0.1)
                            
                            if torch.isnan(loss):
                                loss = torch.tensor(0.0, device=X.device)
                            
                            loss.backward()
                            torch.nn.utils.clip_grad_norm_([layer.weight, layer.bias], max_norm=1.0)
                            optimizer.step()
                            
                            epoch_loss += loss.item()
                        
                        avg_loss = epoch_loss / max(n_batches, 1)
                        block_losses.append(avg_loss)
                        
                        if (epoch + 1) % 5 == 0 or epoch == 0:
                            print(f"    Epoch {epoch+1}/{epochs_per_block} - Loss: {avg_loss:.6f}")
                    
                    results['decoder_results'].append({'losses': block_losses})
                
                return results
            
            # Run custom FF training
            try:
                ff_results = custom_ff_training(
                    model_obj, 
                    X_train_normalized,
                    blocks=getattr(model_obj, 'ff_blocks', 2),
                    epochs_per_block=getattr(model_obj, 'ff_epochs_per_block', 20),
                    lr=getattr(model_obj, 'ff_lr', 0.005)
                )
                
                # Record losses
                for i, block_result in enumerate(ff_results.get('encoder_results', [])):
                    for epoch, loss in enumerate(block_result.get('losses', [])):
                        if not np.isnan(loss):
                            epoch_loss_data.append({
                                'block': i + 1,
                                'block_type': 'encoder',
                                'epoch': epoch + 1,
                                'loss': float(loss),
                                'training_mode': 'forward_forward'
                            })
                
                for i, block_result in enumerate(ff_results.get('decoder_results', [])):
                    for epoch, loss in enumerate(block_result.get('losses', [])):
                        if not np.isnan(loss):
                            epoch_loss_data.append({
                                'block': i + 1,
                                'block_type': 'decoder',
                                'epoch': epoch + 1,
                                'loss': float(loss),
                                'training_mode': 'forward_forward'
                            })
                
                print("Forward Forward training completed successfully!")
                
            except Exception as e:
                print(f"Custom FF training failed: {str(e)}")
                import traceback
                traceback.print_exc()
                use_ff_from_config = False
        
        if hasattr(model_obj, 'use_cafo') and model_obj.use_cafo and use_cafo_from_config:
            print("Using CAFO Cascaded Forward training mode")
            
            try:
                cafo_results = model_obj.train_cafo(X_train, verbose=True)
                
                # Extract loss information from CAFO results
                for i, block_result in enumerate(cafo_results['encoder_results']):
                    for epoch, loss in enumerate(block_result['train_losses']):
                        epoch_loss_data.append({
                            'block': i + 1,
                            'block_type': 'encoder',
                            'epoch': epoch + 1,
                            'loss': float(loss),
                            'training_mode': 'cafo'
                        })
                
                for i, block_result in enumerate(cafo_results['decoder_results']):
                    for epoch, loss in enumerate(block_result['train_losses']):
                        epoch_loss_data.append({
                            'block': i + 1,
                            'block_type': 'decoder',
                            'epoch': epoch + 1,
                            'loss': float(loss),
                            'training_mode': 'cafo'
                        })
                
                print(f"CAFO training completed successfully")
                
            except Exception as e:
                print(f"CAFO training failed: {str(e)}")
                use_cafo_from_config = False
        
        # Traditional VAE training (default or fallback)
        if not use_cafo_from_config and not use_ff_from_config:
            print("Using traditional VAE training mode")
            
            learning_rate = config.get('learning_rate', 0.001)
            epochs = config.get('epochs', 100)
            beta = config.get('beta', 1.0)
            
            optimizer = optim.Adam(model_obj.parameters(), lr=learning_rate)
            
            print(f"Training for {epochs} epochs")
            
            model_obj.train()
            for epoch in range(epochs):
                total_loss = 0
                total_recon_loss = 0
                total_kl_loss = 0
                num_batches = 0
                
                for batch_data in train_loader_obj:
                    if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 1:
                        data = batch_data[0]
                    else:
                        data = batch_data
                    
                    data = data.to(device)
                    optimizer.zero_grad()
                    
                    recon_data, mu, logvar = model_obj(data)
                    
                    recon_loss = nn.MSELoss()(recon_data, data)
                    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                    kl_loss = kl_loss / data.size(0)
                    
                    loss = recon_loss + beta * kl_loss
                    
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                    total_recon_loss += recon_loss.item()
                    total_kl_loss += kl_loss.item()
                    num_batches += 1
                
                if num_batches > 0:
                    avg_loss = total_loss / num_batches
                    avg_recon_loss = total_recon_loss / num_batches
                    avg_kl_loss = total_kl_loss / num_batches
                    
                    print(f"Epoch [{epoch+1}/{epochs}] - Total Loss: {avg_loss:.6f}")
                    
                    epoch_loss_data.append({
                        'epoch': epoch + 1,
                        'loss': avg_loss,
                        'recon_loss': avg_recon_loss,
                        'kl_loss': avg_kl_loss,
                        'training_mode': 'traditional'
                    })

        # Training completion summary
        training_mode = 'forward_forward' if (hasattr(model_obj, 'use_forward_forward') and model_obj.use_forward_forward and use_ff_from_config) else \
                        'cafo' if (hasattr(model_obj, 'use_cafo') and model_obj.use_cafo and use_cafo_from_config) else 'traditional'
        
        print(f"\\nVAE Model Training Completed - Mode: {training_mode.upper()}")
        print(f"Loss entries recorded: {len(epoch_loss_data)}")

        # Save trained model
        output_dir_trained_model = os.path.dirname(args.trained_model)
        if output_dir_trained_model and not os.path.exists(output_dir_trained_model):
            os.makedirs(output_dir_trained_model, exist_ok=True)
        with open(args.trained_model, 'wb') as f:
            pickle.dump(model_obj.cpu(), f)

        # Add UIDs to the epoch loss entries
        for entry in epoch_loss_data:
            entry['uid'] = str(uuid.uuid4())

        output_dir_epoch_loss = os.path.dirname(args.epoch_loss)
        if output_dir_epoch_loss and not os.path.exists(output_dir_epoch_loss):
            os.makedirs(output_dir_epoch_loss, exist_ok=True)
        with open(args.epoch_loss, 'w') as f:
            json.dump(epoch_loss_data, f, indent=2)

        print(f"Saved trained VAE model to {args.trained_model}")
        print(f"Saved training summary to {args.epoch_loss}")
        print(f"Training method used: {training_mode.upper()}")
    args:
      - --model
      - {inputPath: model}
      - --train_loader
      - {inputPath: train_loader}
      - --config
      - {inputValue: config}
      - --trained_model
      - {outputPath: trained_model}
      - --epoch_loss
      - {outputPath: epoch_loss}
